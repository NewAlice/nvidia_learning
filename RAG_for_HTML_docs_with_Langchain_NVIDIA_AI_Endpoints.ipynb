{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60cf18e",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b08f7",
   "metadata": {},
   "source": [
    "# 为 NVIDIA Triton 文档网站构建 RAG 链\n",
    "\n",
    "在这个 notebook 中，我们演示了如何使用 [NVIDIA AI Endpoints for LangChain](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints) 构建一个 RAG。我们通过下载网页并使用 FAISS 生成它们的嵌入来创建一个向量存储。接着，我们展示了两种不同的聊天链用于查询向量存储。这个例子使用的是 NVIDIA Triton 文档网站，不过代码可以很容易地修改为使用其它来源。  \n",
    "\n",
    "### 第一阶段是从网络加载 NVIDIA Triton 文档，分块数据，并使用 FAISS 生成嵌入\n",
    "\n",
    "要运行这个 notebook，您需要完成[设置](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints#setup)并生成一个 API 密钥。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca054393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09431327",
   "metadata": {},
   "source": [
    "运行下面的单元提供 API 密钥。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db2de6",
   "metadata": {},
   "source": [
    "用于加载 html 文件的辅助函数，我们将用它来生成嵌入。稍后会用到这个来从 Triton 文档网站加载相关的 html 文档并转换为向量存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb759b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a document from a given URL and return it's content.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the document.\n",
    "\n",
    "    Returns:\n",
    "        The content of the document.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while making the HTTP request.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Beautiful Soup object to parse html\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get the plain text from the HTML document\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Remove excess whitespace and newlines\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6ccce",
   "metadata": {},
   "source": [
    "读取 html 文件并拆分文本以准备生成嵌入。\n",
    "注意 chunk_size 值必须与用于生成嵌入的特定 LLM 匹配\n",
    "\n",
    "确保关注 TextSplitter 中的 chunk_size 参数。设置合适的 chunk 大小对 RAG 的性能至关重要，因为 RAG 成功的很大一部分依赖于检索步骤找到生成的正确上下文。整个提示词（检索到的块 + 用户查询）必须适合 LLM 的上下文窗口。因此，不应指定过大的块大小，并且要与估计的查询大小保持平衡。例如，虽然 OpenAI LLM 的上下文窗口为 8k-32k tokens，但 Llama3 限制在 8k tokens。可以尝试不同的块大小，但典型的值应该在 100-600 之间，这取决于 LLM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69004fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(embedding_path: str = \"./data/nv_embedding\"):\n",
    "\n",
    "    embedding_path = \"./data/nv_embedding\"\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    # List of web pages containing NVIDIA Triton technical documentation\n",
    "    urls = [\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_analyzer.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html\",\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        document = html_document_loader(url)\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    )\n",
    "    texts = text_splitter.create_documents(documents)\n",
    "    index_docs(url, text_splitter, texts, embedding_path)\n",
    "    print(\"Generated embedding successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ea7a5",
   "metadata": {},
   "source": [
    "使用 NVIDIA AI Endpoints for LangChain 生成嵌入，并将嵌入保存到 ./data/nv_embedding 目录的离线向量存储中，以便将来复用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab26737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_docs(url: Union[str, bytes], splitter, documents: List[str], dest_embed_dir) -> None:\n",
    "    \"\"\"\n",
    "    Split the document into chunks and create embeddings for the document\n",
    "\n",
    "    Args:\n",
    "        url: Source url for the document.\n",
    "        splitter: Splitter used to split the document\n",
    "        documents: list of documents whose embeddings needs to be created\n",
    "        dest_embed_dir: destination directory for embeddings\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")\n",
    "\n",
    "    for document in documents:\n",
    "        texts = splitter.split_text(document.page_content)\n",
    "\n",
    "        # metadata to attach to document\n",
    "        metadatas = [document.metadata]\n",
    "\n",
    "        # create embeddings and add to vector store\n",
    "        if os.path.exists(dest_embed_dir):\n",
    "            update = FAISS.load_local(folder_path=dest_embed_dir, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "            update.add_texts(texts, metadatas=metadatas)\n",
    "            update.save_local(folder_path=dest_embed_dir)\n",
    "        else:\n",
    "            docsearch = FAISS.from_texts(texts, embedding=embeddings, metadatas=metadatas)\n",
    "            docsearch.save_local(folder_path=dest_embed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac12dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ae7e2",
   "metadata": {},
   "source": [
    "### 第二阶段是加载向量存储中的嵌入并使用 NVIDIAEmbeddings 构建 RAG\n",
    "\n",
    "通过 NVIDIA Retrieval QA 嵌入端点创建嵌入模型。这个模型将单词、短语或其它实体表示为数字向量，并理解单词和短语之间的关系。详细信息请参考： https://build.nvidia.com/nvidia/embed-qa-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09613194",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\", allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3600160",
   "metadata": {},
   "source": [
    "使用 FAISS 从向量数据库加载文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "embedding_path = \"./data/nv_embedding\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should return documents related to the test query\n",
    "retriever.invoke(\"Deploy TensorRT-LLM Engine on Triton Inference Server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df16def",
   "metadata": {},
   "source": [
    "创建一个 ConversationalRetrievalChain 链。在这个链中，我们演示了如何使用两个 LLM：一个用于摘要，另一个用于对话。这在更复杂的场景中提高了整体结果。我们将使用 Llama3 70B 作为第一个 LLM，Mixtral 作为链中的对话元素。我们添加一个 question_generator 来生成相关的查询提示词。详细信息请参考： https://python.langchain.com/docs/modules/chains/popular/chat_vector_db#conversationalretrievalchain-with-streaming-to-stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{CONDENSE_QUESTION_PROMPT = }\")\n",
    "print(f\"{QA_PROMPT = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model='mistralai/mixtral-8x7b-instruct-v0.1')\n",
    "chat = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "## Requires question and chat_history\n",
    "qa_chain = (RunnablePassthrough()\n",
    "    ## {question, chat_history} -> str\n",
    "    | CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()\n",
    "    # | RunnablePassthrough(print)\n",
    "    ## str -> {question, context}\n",
    "    | {\"question\": lambda x: x, \"context\": retriever}\n",
    "    # | RunnablePassthrough(print)\n",
    "    ## {question, context} -> str\n",
    "    | QA_PROMPT | chat | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b06cf7",
   "metadata": {},
   "source": [
    "问任何关于 Triton 的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e89db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Triton?\"\n",
    "chat_history += [qa_chain.invoke({\"question\": query, \"chat_history\": chat_history})]\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d30d2",
   "metadata": {},
   "source": [
    "再问一个关于 Triton 的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What interfaces does Triton support?\"\n",
    "chat_history += [\"\"]\n",
    "for token in qa_chain.stream({\"question\": query, \"chat_history\": chat_history[:-1]}):\n",
    "    print(token, end=\"\")\n",
    "    chat_history[-1] += token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354c3df",
   "metadata": {},
   "source": [
    "最后通过询问之前的查询来展示聊天能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"But why?\"\n",
    "for token in qa_chain.stream({\"question\": query, \"chat_history\": chat_history}):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1932a3",
   "metadata": {},
   "source": [
    "现在我们展示一个更简单的链，仅使用一个 LLM，即聊天 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatNVIDIA(\n",
    "    model='mistralai/mixtral-8x7b-instruct-v0.1', \n",
    "    temperature=0.1, \n",
    "    max_tokens=1000, \n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \n",
    "        \"Use the following pieces of context to answer the question at the end.\"\n",
    "        \" If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "        \"\\n\\nHISTORY: {history}\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\n",
    "    )\n",
    "])\n",
    "\n",
    "## Requires question and chat_history\n",
    "qa_chain = (\n",
    "    RunnablePassthrough.assign(context = (lambda state: state.get(\"question\")) | retriever)\n",
    "    # | RunnablePassthrough(print)\n",
    "    | qa_prompt | chat | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e125e",
   "metadata": {},
   "source": [
    "现在尝试用更简单的链询问关于 Triton 的问题。将答案与之前复杂链模型的结果进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001da5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Triton?\"\n",
    "chat_history += [qa_chain.invoke({\"question\": query, \"history\": chat_history})]\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e53383",
   "metadata": {},
   "source": [
    "再问一个关于 Triton 的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Does Triton support ONNX?\"\n",
    "chat_history += [\"\"]\n",
    "for token in qa_chain.stream({\"question\": query, \"history\": chat_history[:-1]}):\n",
    "    print(token, end=\"\")\n",
    "    chat_history[-1] += token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b586d97",
   "metadata": {},
   "source": [
    "最后通过询问之前的查询来展示聊天能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e48df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How come?\"\n",
    "for token in qa_chain.stream({\"question\": query, \"history\": chat_history}):\n",
    "    print(token, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
