{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为大规模推理部署模型\n",
    "\n",
    "## 01 - 新手入门\n",
    "\n",
    "-------\n",
    "\n",
    "**目录**\n",
    "\n",
    "* [简介](#introduction)\n",
    "* [Triton 推理服务器](#triton)\n",
    "* [设置](#setup)\n",
    "* [结语](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### 简介\n",
    "\n",
    "在此 notebook 中，我们将介绍 Triton 推理服务器是什么，并为实验执行一些简单设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"triton\"></a>\n",
    "### Triton 推理服务器\n",
    "\n",
    "NVIDIA Triton 推理服务器简化了在生产环境中大规模部署 AI 模型。Triton 是一款开源的推理服务软件，可助力您的团队从任何框架、从本地存储或从云端（Google Cloud 或 Azure）、在数据中心或边缘、在基于 GPU 或 CPU 的基础设施上部署经过训练的 AI 模型。您可从 NVIDIA NGC 目录中拉取它的容器并可迅速启用 Triton。NGC 是深度学习和机器学习软件中心，这些软件均经由 GPU 优化过，供您加速向开发工作流程进行部署。\n",
    "\n",
    "下图展示了 Triton 推理服务器的总体架构。模型资源库是基于文件系统的模型库，Triton 可使用其中的模型执行推理。在推理请求通过 HTTP/REST 或 GRPC 或 C API 到达服务器后，系统会将这些请求送入与各个模型对应的调度程序中。Triton 可实现多个调度和批量处理算法，支持按模型进行配置。每个模型的调度程序可视情况批量处理推理请求，然后将请求传递至与模型类型对应的后端。后端使用批量请求所提供的输入执行推理，以生成请求的输出。然后返回输出。\n",
    "\n",
    "<img src=\"./assets/A-schematic-of-Triton-Server-architecture.png\" alt=\"A Schematic of Triton Inference Server\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "### 设置\n",
    "\n",
    "首先，检查系统里有什么样的 GPU："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如我们所见，系统中有 1 个 GPU，即 Tesla T4。\n",
    "\n",
    "然后，检查文件系统："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到多个文件夹和 Jupyter notebook。稍后我们将在实验中将使用这些 notebook。最后，检查使用中的 CUDA 版本。从以下输出中可以得知我们正在使用 CUDA 11.1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**服务器**\n",
    "\n",
    "在本实验中，我们已运行了 Triton 推理服务器实例。运行 Triton 服务器实例的代码如下所示。如需了解详细信息，请参阅快速入门和构建说明：\n",
    "\n",
    "* [快速入门文档](https://github.com/triton-inference-server/server/blob/r20.12/docs/quickstart.md)\n",
    "* [构建文档](https://github.com/triton-inference-server/server/blob/r20.12/docs/build.md)\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "  --gpus=1 \\\n",
    "  --ipc=host --rm \\\n",
    "  --shm-size=1g \\\n",
    "  --ulimit memlock=-1 \\\n",
    "  --ulimit stack=67108864 \\\n",
    "  -p 8000：8000 -p 8001：8001 -p 8002：8002 \\\n",
    "  -v /models：/models \\\n",
    "  nvcr.io/nvidia/tritonserver:20.12-py3 \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/models \\\n",
    "  --exit-on-error=false \\\n",
    "  --model-control-mode=poll \\\n",
    "  --repository-poll-secs 30\n",
    "```\n",
    "\n",
    "有关 Triton 推理服务器的容器的相关内容，请查看：https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver\n",
    "\n",
    "**客户端**\n",
    "\n",
    "我们还安装了 Triton 推理服务器的客户端库，这些库可提供编程接口，从而方便您通过 C++ 或 Python 应用与 Triton 通信。使用这些库，您可以向 Triton 发送 HTTP/REST 或 GRPC 请求，访问 Triton 的所有功能：推理、状态和运行情况、统计数据和运行性能、模型库管理等。这些库还支持使用系统和 CUDA 的共享内存，将输入传递至 Triton 并接收输出。本课程中的示例演示了 C++ 和 Python 库的用法。\n",
    "\n",
    "获取 Python 客户端库的简便方法是使用 PIP 安装 `tritonclient` 模组，详情如下。如需详细了解如何下载或构建 Triton 推理服务器客户端库，您可以在此处查看相关文档：https://github.com/triton-inference-server/server/blob/r20.12/docs/client_libraries.md\n",
    "\n",
    "```\n",
    "pip install nvidia-pyindex\n",
    "pip install tritonclient[all]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### 小结\n",
    "\n",
    "在此 notebook 中，我们概述了 Triton 推理服务器的功能，并为实验执行了一些简单设置。欢迎继续学习下一个 notebook！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
