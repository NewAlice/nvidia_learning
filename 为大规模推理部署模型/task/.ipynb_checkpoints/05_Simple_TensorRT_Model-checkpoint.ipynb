{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为大规模推理部署模型\n",
    "\n",
    "## 05 - 简单的 TensorRT 模型\n",
    "-------\n",
    "\n",
    "**目录**\n",
    "\n",
    "* [简介](#introduction)\n",
    "* [TensorRT - 可编程的推理加速器](#tensorrt)\n",
    "* [创建模型目录结构](#structure)\n",
    "* [将 ONNX 转换为 TensorRT](#model)\n",
    "* [创建配置文件](#configuration)\n",
    "* [在 Triton 推理服务器中加载模型](#load)\n",
    "* [将推理请求发送到服务器](#infer)\n",
    "* [练习](#exercise)\n",
    "* [小结](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### 简介\n",
    "\n",
    "在此 notebook 中，我们将采用之前创建的 ONNX 模型，并将其转换为可在 Triton 推理服务器中部署的 TensorRT 表示方式。作为经过高度优化的软件包，TensorRT 对训练过的模型以推理性能为目的而实施优化。我们将了解如何在 Triton 推理服务器内创建模型目录结构和配置文件、如何使用 `trtexec`，以及如何向部署在 Triton 推理服务器中的模型发送推理请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tensorrt\"></a>\n",
    "### TensorRT - 可编程的推理加速器\n",
    "\n",
    "NVIDIA TensorRT™ 是一个高性能深度学习推理平台。它包含深度学习推理优化器，以及可为深度学习推理应用提供低延迟和高吞吐量的运行时（runtime）。在推理过程中，基于 TensorRT 的应用在执行速度方面可比仅采用 CPU 的平台快 40 倍。\n",
    "\n",
    "借助 TensorRT，您可以优化在所有主要框架中训练出来的神经网络模型，精确地校正低精度量化，并最终将模型部署到超大规模数据中心、嵌入式或汽车产品平台上。\n",
    "\n",
    "以下是一些有关 TensorRT 入门的实用资源：\n",
    " \n",
    "* 主页：https://developer.nvidia.cn/zh-cn/tensorrt\n",
    "* 博客：https://devblogs.nvidia.cn/speed-up-inference-tensorrt/\n",
    "* 下载：https://developer.nvidia.cn/nvidia-tensorrt-download\n",
    "* 文档：https://docs.nvidia.cn/deeplearning/sdk/tensorrt-developer-guide/index.html\n",
    "* 示例代码：https://docs.nvidia.cn/deeplearning/sdk/tensorrt-sample-support-guide/index.html\n",
    "* GitHub：https://github.com/NVIDIA/TensorRT\n",
    "* NGC 容器：https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structure\"></a>\n",
    "### 创建模型目录结构\n",
    "\n",
    "Triton 推理服务器可为模型库中的模型提供服务。首次运行 Triton 推理服务器时，您需要指定模型所在的模型库位置：\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "每个模型都位于模型库内对应的模型子目录中，即 `/models` 中的每个目录均表示一个唯一的模型。例如，在此 notebook 中，我们将在 `simple-tensorrt-fp32-model` 和`simple-tensorrt-fp16-model` 中部署两个 TensorRT 模型，将其中一个优化为使用 FP32 数据类型，另一个优化为使用 FP16 数据类型。\n",
    "\n",
    "所有模型通常遵循相似的目录结构。在每个目录中，我们将创建配置文件 `config.pbtxt`，用它来详细介绍模型信息，例如批量大小、输入形状、部署后端（比如 PyTorch、ONNX、TensorFlow、TensorRT）等等。稍后，我们将在此 notebook 中探讨配置文件。\n",
    "\n",
    "此外，我们还可以创建一个或多个模型版本。每个版本都位于具有相应版本号（始于 `1`）的子目录名称之下。配置文件位于我们模型文件所在的此子目录内（例如 `model.plan`）。\n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "└── simple-tensorrt-model\n",
    "    ├── 1\n",
    "    │   └── model.plan\n",
    "    └── config.pbtxt\n",
    "\n",
    "```\n",
    "\n",
    "我们还可以添加一个文件以表示输出名称。为简洁起见，我们已在此 notebook 中省略了此步骤。如需详细了解如何在 Triton 推理服务器中使用模型库和模型目录结构，请在此参阅以下文档：https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md\n",
    "\n",
    "下面，我们将为每个 TensorRT 模型创建模型目录结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/simple-tensorrt-fp32-model/\n",
    "!mkdir -p models/simple-tensorrt-fp32-model/1/\n",
    "!mkdir -p models/simple-tensorrt-fp16-model/\n",
    "!mkdir -p models/simple-tensorrt-fp16-model/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### 将 ONNX 转换为 TensorRT\n",
    "\n",
    "在之前的 [02_Simple_PyTorch_Model.ipynb](02_Simple_PyTorch_Model.ipynb) notebook 中，我们以 ONNX 的表示方式创建了 ResNet50 模型，并将其保存为 `model.onnx` 文件（如果您在上一个会话过期后返回此 notebook，请重新运行 notebook 02）。在本节中，我们将采用该 ONNX 表示方式，并使用 `trtexec` 命令行实用程序将其转换为 TensorRT plan 文件。`trtexec` 简单易用，有关安装和文档的详细信息，请参阅此处：https://github.com/NVIDIA/TensorRT/tree/master/samples/trtexec\n",
    "\n",
    "要详细了解该 `trtexec` 命令，请取消注释并执行以下单元（警告：下列单元十分冗长）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec \\\n",
    "  --onnx=models/simple-onnx-model/1/model.onnx \\\n",
    "  --explicitBatch \\\n",
    "  --optShapes=actual_input_1:16x3x224x224 \\\n",
    "  --maxShapes=actual_input_1:32x3x224x224 \\\n",
    "  --minShapes=actual_input_1:1x3x224x224 \\\n",
    "  --shapes=actual_input_1:1x3x224x224 \\\n",
    "  --saveEngine=models/simple-tensorrt-fp32-model/1/model.plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要将 ONNX 表示方式转换为 TensorRT 的 plan 文件，我们需指向 `model.onnx` 文件，并为我们新创建的 TensorRT plan 文件指定输出。就是这么简单！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过添加 `--fp16` 标志，我们可以指定针对 FP16 优化 TensorRT plan 文件。使用 FP16 有很多好处，主要是该类数据速度更快（计算量更少）并且内存占用更少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec \\\n",
    "  --onnx=models/simple-onnx-model/1/model.onnx \\\n",
    "  --explicitBatch \\\n",
    "  --optShapes=actual_input_1:16x3x224x224 \\\n",
    "  --maxShapes=actual_input_1:32x3x224x224 \\\n",
    "  --minShapes=actual_input_1:1x3x224x224 \\\n",
    "  --shapes=actual_input_1:1x3x224x224 \\\n",
    "  --saveEngine=models/simple-tensorrt-fp16-model/1/model.plan \\\n",
    "  --fp16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "### 创建配置文件\n",
    "\n",
    "以 TensorRT plan 表示方法定义和写出模型后，我们现在将注意力转向为模型创建配置文件。\n",
    "\n",
    "对于模型配置而言，至少须指定模型名称、平台或后端属性、最大批量（max_batch_size） 属性以及模型的输入和输出张量（名称、数据类型和形状）。\n",
    "\n",
    "如需详细了解如何在 Triton 推理服务器中创建模型配置文件，请参阅相关文档：\n",
    "https://github.com/triton-inference-server/server/blob/r20.12/docs/model_configuration.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-tensorrt-fp32-model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-tensorrt-fp32-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还将为 TensorRT fp16 模型创建配置文件。请注意，我们的输入和输出数据类型仍会保留在 FP32 表示形式中，尽管我们神经网络的内部层和激活函数将使用 FP16 数据类型，但我们的输入和输出数据仍将以 FP32 形式显示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-tensorrt-fp16-model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-tensorrt-fp16-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### 在 Triton 推理服务器中加载模型\n",
    "\n",
    "\n",
    "创建模型目录结构、定义和导出模型以及创建配置文件后，我们现在将等待 Triton 推理服务器来加载模型。我们设置此实验以在**轮询**模式下使用 Triton 推理服务器。这意味着 Triton 推理服务器将以 30 秒为间隔，持续轮询模型的修改内容或新创建的模型。请运行以下单元，以预留一些时间，以便 Triton 推理服务器对新模型/修改内容进行轮询，然后再继续下一步操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，我们的模型应已部署就绪且随时可用！为确认 Triton 推理服务器已启动并运行，我们会看到对以下 URL 的 `curl` 请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 Triton 已准备就绪，则 HTTP 请求会返回状态 200；如果 Triton 未准备就绪，则会返回 200 以外的状态。\n",
    "\n",
    "我们还可以向模型端点发送 `curl` 请求，以确认我们的模型已部署就绪并可随时使用。如果模型已准备就绪，则 `curl` 请求将返回状态 200；如果尚未准备就绪，则返回 200 以外的状态。\n",
    "\n",
    "此外，我们还将看到模型的相关信息，例如：\n",
    "\n",
    "* 模型的名称、\n",
    "* 模型可用的版本、\n",
    "* 后端平台（例如 tensorrt_plan）、\n",
    "* 附带各自名称、数据类型和形状的输入与输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-tensorrt-fp32-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-tensorrt-fp16-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer\"></a>\n",
    "### 将推理请求发送到服务器\n",
    "\n",
    "模型部署就绪后，即可向模型发送推理请求。\n",
    "\n",
    "首先，我们将加载 `tritonclient.http` 模组和实用程序函数，用于处理 NumPy 数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from tritonclient.utils import triton_to_np_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将加载 ImageNet 输入标签、金鱼图像以及 TensorFlow Keras 中用于加载图像、调整图像大小、预处理输入和解码预测的一些帮助函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./imagenet-simple-labels.json') as file:\n",
    "    labels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "img_path = './assets/goldfish.jpg'\n",
    "image_pil = Image.open(img_path)\n",
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "image_numpy = image.img_to_array(img)\n",
    "image_numpy = np.expand_dims(image_numpy, axis=0)\n",
    "image_numpy = preprocess_input(image_numpy)\n",
    "image_numpy = np.transpose(image_numpy, [0, 3, 1, 2]) / 255.\n",
    "print(image_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将定义模型的输入和输出名称、模型名称、使用 Triton 推理服务器向其中部署模型的 URL（本例中为 `triton:8000` 的本地主机）以及模型版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'actual_input_1'\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-tensorrt-fp32-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用 `tritonhttpclient.InferenceServerClient` 类通过 `.get_model_metadata()` 方法访问模型元数据，并使用 `get_model_config()` 方法获取模型配置，进而实例化客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用预期输入名称、形状和数据类型来实例化输入数据的占位符。将金鱼图像的输入数据设置为 NumPy 数组表示形式。还需仅使用输出名称实例化输出数据的占位符。\n",
    "\n",
    "最后，我们将使用 `triton_client.infer()` 方法将输入提交至 Triton 推理服务器，指定模型名称、模型版本、输入和输出，并将结果转换为 NumPy 数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, (1, 3, 224, 224), 'FP32')\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy('output1')\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所需的全部操作如上所述！我们可以识别最大的 logit 值，并确认我们的模型正确推断出图像实际上是一条金鱼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise\"></a>\n",
    "### 练习 #2：向 TensorRT FP16 模型提交推理请求\n",
    "\n",
    "我们将为参加本课程的学员布置一份练习：向已部署的 TensorRT Fp16 模型提交推理请求。如果遇到问题（或想确认答案），请单击 `...` 以显示答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 1 步：定义名称和形状\n",
    "\n",
    "**提示**：试着查看上文定义的 TensorRT Fp16 `configuration`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = FIXME\n",
    "input_name = FIXME\n",
    "input_shape = FIXME\n",
    "input_dtype = FIXME\n",
    "output_name = FIXME\n",
    "model_name = FIXME\n",
    "url = FIXME\n",
    "model_version = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'actual_input_1'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-tensorrt-fp16-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 2 步：从 Triton 获取模型信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = FIXME\n",
    "model_metadata = FIXME\n",
    "model_config = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 3 步：定义输入和输出以从 Triton 获取推理响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = FIXME\n",
    "\n",
    "output = FIXME\n",
    "response = FIXME\n",
    "\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 4 步：验证响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### 小结\n",
    "\n",
    "在此 notebook 中，我们展示了如何采用之前创建的 ONNX 模型，并将其转换为可在 Triton 推理服务器中部署的 TensorRT 表示方式。我们了解了如何在 Triton 推理服务器内创建模型目录结构和配置文件、如何使用 `trtexec`，以及如何向部署在 Triton 推理服务器中的模型发送推理请求。\n",
    "\n",
    "我们建议您运行下面的单元进行清理。此操作将释放 GPU 显存，以供实验的其他部分使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
