{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为大规模推理部署模型\n",
    "\n",
    "## 02 - 简单的 PyTorch 模型\n",
    "-------\n",
    "\n",
    "**目录**\n",
    "\n",
    "* [简介](#introduction)\n",
    "* [创建模型目录结构](#structure)\n",
    "* [定义一个简单的 PyTorch 模型](#model)\n",
    "* [使用 TorchScript 追踪模型](#torchscript)\n",
    "* [创建配置文件](#configuration)\n",
    "* [在 Triton 推理服务器中加载模型](#load)\n",
    "* [将推理请求发送到服务器](#infer)\n",
    "* [练习](#exercise)\n",
    "* [小结](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### 简介\n",
    "\n",
    "我们将在此 notebook 中创建 PyTorch ResNet50 模型，将其写为本地 PyTorch 模型并转换为 ONNX 表示形式，然后使用 Triton 推理服务器加以部署。我们将了解如何在 Triton 推理服务器中创建模型目录结构和配置文件、如何使用 TorchScript 和 ONNX，以及如何向 Triton 推理服务器中部署的模型发送推理请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structure\"></a>\n",
    "### 创建模型目录结构\n",
    "\n",
    "Triton 推理服务器可为模型库中的模型提供服务。首次运行 Triton 推理服务器时，您需要指定模型所在的模型库位置：\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "每个模型都位于模型库内各自的模型子目录中，即 `/models` 下的每个目录均表示一个唯一的模型。例如，我们将在此 notebook 中部署两个模型：`simple-onnx-model` 和 `simple-pytorch-model`。\n",
    "\n",
    "所有模型通常遵循相似的目录结构。在每个目录中，我们将创建配置文件 `config.pbtxt`，用它来详细描述模型的有关信息，例如批量大小、输入张量的形状、部署用的后端（比如 PyTorch、ONNX、TensorFlow、TensorRT）等等。稍后，我们将在此 notebook 中探索配置文件。\n",
    "\n",
    "此外，我们还可以创建一个或多个模型版本。每个版本都位于具有相应版本号（始于 `1`）的子目录名称之下，而模型文件（例如 `model.onnx`、`model.pt`）就在这个子目录下。\n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "├──imple-onnx-model\n",
    "│   ├── 1\n",
    "│   │   └── model.onnx\n",
    "│   └── config.pbtxt\n",
    "├── simple-pytorch-model\n",
    "│   ├── 1\n",
    "│   │   └── model.pt\n",
    "│   └── config.pbtxt\n",
    "\n",
    "```\n",
    "\n",
    "我们还可以添加一个文件以表示输出名称。为简洁起见，我们已在此 notebook 中省略了此步骤。如需详细了解如何在 Triton 推理服务器中使用模型库和模型目录结构，请参阅以下文档：https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md\n",
    "\n",
    "接下来，我们将为每个 PyTorch 和 ONNX 模型创建模型目录结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/simple-pytorch-model\n",
    "!mkdir -p models/simple-pytorch-model/1\n",
    "!mkdir -p models/simple-onnx-model\n",
    "!mkdir -p models/simple-onnx-model/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### 定义一个简单的 PyTorch 模型\n",
    "\n",
    "在下一节中，我们将定义一个简单的 PyTorch ResNet50 模型。 我们指定将使用的预训练后的 ResNet50 模型，该模型被从 ImageNet 训练中学到的权重所实例化。在定义了我们的 `Model` 类之后，我们将实例化这个模型，使用 `.eval()` 方法将模型设置为评估模式，并使用 `.cuda()` 方法将模型分配到 GPU 上。如需详细了解如何使用 CUDA 在 GPU 上训练 PyTorch 模型，请参阅[此篇文章](https://medium.com/ai%C2%B3-theory-practice-business/use-gpu-in-your-pytorch-code-676a67faed09)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = Model().eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，加载 ImageNet 标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./imagenet-simple-labels.json') as file:\n",
    "    labels = json.load(file)\n",
    "\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 Triton 推理服务器之前，我们需要确认在 ImageNet 上预训练的 ResNet50 模型适用于样本图像。我们将使用金鱼图像，您可以随意尝试使用自己的图像！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image = Image.open('./assets/goldfish.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们将创建一个转换流程来获取图像，将图像大小调整为 `(256, 256)`，进行中心裁剪，生成一张大小为 `(224, 224)` 的图像， 并将其转换为 PyTorch 张量，然后使用 ImageNet 数据集的均值和标准差对图像进行归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.485, 0.456, 0.406]\n",
    "\n",
    "resize = transforms.Resize((256, 256))\n",
    "center_crop = transforms.CenterCrop(224)\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean=imagenet_mean,\n",
    "                                 std=imagenet_std)\n",
    "\n",
    "transform = transforms.Compose([resize, center_crop, to_tensor, normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将对图像应用转换流程，使用 `.unsqueeze(0)` 方法为批量大小添加维度，然后使用 `.cuda()` 方法把图像分配到 GPU 上。我们将让图像通过模型以获得 `logits`输出。\n",
    "\n",
    "将 `logits` 迁移到 CPU 后，我们将使用 `torch.topk` 函数访问前 3 个最大的 `logits` 值和它们的索引。最终得到的结果确实是一条金鱼。太棒了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = transform(image).unsqueeze(0).cuda()\n",
    "logits = model(image_tensor)\n",
    "\n",
    "K = 3\n",
    "values, indices = torch.topk(logits.cpu(), K)\n",
    "\n",
    "values = values.detach().numpy().tolist()[0]\n",
    "indices = indices.detach().numpy().tolist()[0]\n",
    "\n",
    "for i in range(K):\n",
    "    print(values[i], indices[i], labels[indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"torchscript\"></a>\n",
    "### 使用 TorchScript 追踪模型\n",
    "\n",
    "\n",
    "我们已定义模型并确定该模型能如我们所期待地那样运行。在将模型写成 `model.pt` 文件之前，我们将使用 TorchScript 追踪模型。TorchScript 方法可使用 PyTorch 代码创建可序列化且可优化的模型。我们可以保存 Python 进程中的任何 TorchScript 程序，并将这些程序加载到没有 Python 依赖项的进程中。这是我们使用 `libtorch` 后端将 PyTorch 模型加载到 Triton 推理服务器时需要执行的工作。\n",
    "\n",
    "使用 TorchScript 生成模型的方法有两种：使用 `torch.jit.script` 函数或 `torch.jit.trace` 函数。\n",
    "\n",
    "在函数或 `nn.Module` 上使用 `torch.jit.script` 将检查源代码，使用 TorchScript 编译器将其编译为 TorchScript 代码，然后返回 `ScriptModule` 或 `ScriptFunction`。\n",
    "\n",
    "在函数上使用 `torch.jit.trace` 将返回可执行文件或 `ScriptFunction`，这些内容支持使用即时编译加以优化。\n",
    "\n",
    "使用 `torch.jit.script` 还是 `torch.jit.trace` 有待商榷。通常来说，`torch.jit.script` 在灵活方面更胜一筹，可支持您处理不同的批量大小，而 `torch.jit.trace` 要求您使用固定批量大小的示例虚拟输入数据传递给模型。通常，我建议您从 `torch.jit.script` 入手。\n",
    "\n",
    "有关 TorchScript 的更多详情，请参阅：\n",
    "\n",
    "* TorchScript 文档：https://pytorch.org/docs/stable/jit.html\n",
    "* 这篇博文非常富有见地：https://paulbridger.com/posts/mastering-torchscript/\n",
    "\n",
    "下面，我们将围绕模型定义封装器，将模型封装器设置为评估模式，并在 GPU 上分配模型。接下来，我们将使用 `torch.jit.script` 函数生成 TorchScript 代码，在 `simple-pytorch-model` 模型目录的版本 `1` 子目录下，将模型写为 `model.pt`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorch_to_TorchScript(nn.Module):\n",
    "    def __init__(self, my_model):\n",
    "        super(PyTorch_to_TorchScript, self).__init__()\n",
    "        self.model = my_model.model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "torchscript_model = PyTorch_to_TorchScript(model).eval().cuda()\n",
    "traced_script_module = torch.jit.script(torchscript_model)\n",
    "traced_script_module.save('models/simple-pytorch-model/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还会将模型转换为 ONNX 的表示形式。开放的神经网络交换 (ONNX) 是一个开放的生态系统，可助力 AI 开发者在项目开发时选择合适的工具。ONNX 为深度学习和传统机器学习等 AI 模型提供了开源格式。它定义了可扩展的计算图模型，以及内置运算符和标准数据类型。目前，我们侧重于用于推理（评分）所需的能力。\n",
    "\n",
    "下面，我们将基于输入图像的形状创建随机数据的 Torch Tensor，并将其分配至 GPU。我们还将指定模型的输入和输出名称。我们将在下一节中介绍这些值在配置模型中的用法。\n",
    "\n",
    "最后，我们将以 ONNX 表示形式，在 `simple-onnx-model` 模型目录的 `1` 版本子目录下，将模型导出为 `model.onnx` 文件，并指定虚拟输入以及相应的输入和输出名称。我们还将传入一个字典，将输入和输出名称映射为批量大小的维度。我们可借此处理不同的批量大小，如果不使用 `dynamic_axes` 参数，系统将对 ONNX 模型采用硬编码，使其使用我们为虚拟输入选择的任何批量大小（本例中的批量大小为 1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224).cuda()\n",
    "\n",
    "input_names = ['actual_input_1'] + ['learned_%d' % i for i in range(16)]\n",
    "output_names = ['output1']\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \n",
    "                  'models/simple-onnx-model/1/model.onnx', verbose=False, \n",
    "                  input_names=input_names, output_names=output_names, \n",
    "                  dynamic_axes={'actual_input_1': {0: 'batch_size'}, 'output1': {0: 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "### 创建配置文件\n",
    "\n",
    "以 TorchScript 和 ONNX 表示形式定义和写入模型后，我们现在将注意力转向为模型创建配置文件。\n",
    "\n",
    "对于模型配置而言，至少须指定模型名称、平台或后端属性、max_batch_size 属性以及模型的输入和输出张量（名称、数据类型和形状）。\n",
    "\n",
    "\n",
    "如需详细了解如何在 Triton 推理服务器中创建模型配置文件，请参阅相关文档：\n",
    "https://github.com/triton-inference-server/server/blob/r20.12/docs/model_configuration.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-pytorch-model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-pytorch-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还将为 ONNX 模型创建配置文件。请注意，因为我们在导出 ONNX 模型时指定了输入和输出名称，所以输入和输出张量的名称属性不同。请注意，`platform` 已更新为 `onnxruntime_onnx`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-onnx-model\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-onnx-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### 在 Triton 推理服务器中加载模型\n",
    "\n",
    "\n",
    "创建模型目录结构、定义和导出模型以及创建配置文件后，我们现在将等待 Triton 推理服务器来加载模型。我们已将此实验设置成**轮询**模式下使用 Triton 推理服务器。这意味着 Triton 推理服务器将以 30 秒为间隔，持续轮询模型的修改内容或新创建的模型。请运行以下单元，以预留一些时间，以便 Triton 推理服务器对新模型/修改内容进行轮询，然后再继续下一步操作。由于此步骤的异步性质，我们增加了 15 秒时间以确保顺利完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，我们的模型应已部署就绪且随时可用！为确认 Triton 推理服务器已启动并运行，我们会看到对以下 URL 的 `curl` 请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 Triton 已准备就绪，则 HTTP 请求会返回状态 200；如果 Triton 未准备就绪，则会返回 200 以外的状态。\n",
    "\n",
    "我们还可以向模型端点发送 `curl` 请求，以确认我们的模型已部署就绪并可随时使用。如果模型已准备就绪，此 `curl` 请求会返回状态 200；如果模型未准备就绪，则会返回 200 以外的状态。\n",
    "\n",
    "此外，我们还将看到模型的相关信息，例如：\n",
    "\n",
    "* 模型的名称、\n",
    "* 模型可用的版本、\n",
    "* 后端平台（例如 pytorch_libtorch、onnxruntime_onnx）、\n",
    "* 附带各自名称、数据类型和形状的输入与输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-pytorch-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-onnx-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer\"></a>\n",
    "### 将推理请求发送到服务器\n",
    "\n",
    "模型部署就绪后，即可向模型发送推理请求。\n",
    "\n",
    "首先，我们将加载 `tritonclient.http` 模组和实用程序函数，用于处理 NumPy 数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将定义模型的输入和输出名称、模型名称、使用 Triton 推理服务器向其中部署模型的 URL（本例中为主机`triton:8000`）以及模型版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'input__0'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output__0'\n",
    "model_name = 'simple-pytorch-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用 `tritonhttpclient.InferenceServerClient` 类通过 `.get_model_metadata()` 方法访问模型元数据，并使用 `get_model_config()` 方法获取模型配置，进而实例化客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将金鱼（当前为 Torch Tensor）的预定义图像转换为 CPU 上的 NumPy 数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numpy = image_tensor.cpu().numpy()\n",
    "print(image_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用输入名称、形状和期望的数据类型来实例化输入数据的占位符（placeholder）。我们将金鱼图像的输入数据设置为 NumPy 数组表示形式，还需仅用名字来实例化输出数据的占位符。\n",
    "\n",
    "最后，我们将使用 `triton_client.infer()` 方法将输入提交至 Triton 推理服务器，指定模型名称、模型版本、输入和输出，并将结果转换为 NumPy 数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所需的全部操作如上所述！我们可以识别最大的 logit 值，并确认我们的模型正确推断出图像确实是一条金鱼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise\"></a>\n",
    "### 练习 #1 - 向 ONNX 模型提交推理请求\n",
    "\n",
    "我们为学员布置一个练习：向已部署就绪的 ONNX 模型提交推理请求。如果遇到问题（或想确认答案），请单击 `...` 以显示答案。\n",
    "\n",
    "提示：仅复制上述推理代码不起作用，请注意我们为 ONNX 定义的配置文件中的模型名称和输入和输出名称。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 1 步：定义名称和形状\n",
    "\n",
    "**提示**：尝试查看上面定义的 ONNX `configuration`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = FIXME\n",
    "input_name = FIXME\n",
    "input_shape = FIXME\n",
    "input_dtype = FIXME\n",
    "output_name = FIXME\n",
    "model_name = FIXME\n",
    "url = FIXME\n",
    "model_version = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'actual_input_1'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-onnx-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 2 步：从 Triton 获取模型信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.FIXME(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.FIXME(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.FIXME(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 3 步：测试图像\n",
    "\n",
    "这里没有 `FIXME`，查看图像形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numpy = image_tensor.cpu().numpy()\n",
    "print(image_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 4 步：定义输入和输出以从 Triton 获取推理响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = FIXME\n",
    "\n",
    "output = FIXME\n",
    "response = FIXME\n",
    "\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第 5 步：验证响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### 小结\n",
    "\n",
    "我们在此 notebook 中展示了如何创建 PyTorch ResNet50 模型，将其写为本地 PyTorch 模型并转换为 ONNX 表示形式，然后使用 Triton 推理服务器加以部署。我们了解了如何在 Triton 推理服务器中创建模型目录结构和配置文件、如何使用 TorchScript 和 ONNX，以及如何向 Triton 推理服务器中部署的模型发送推理请求。\n",
    "\n",
    "我们建议您运行下面的单元进行清理工作，此操作将释放 GPU 显存，以供实验的其它部分使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
