{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为大规模推理部署模型\n",
    "\n",
    "## 03 - HuggingFace 模型\n",
    "-------\n",
    "\n",
    "**目录**\n",
    "\n",
    "* [简介](#introduction)\n",
    "* [定义 HuggingFace 预训练模型](#model)\n",
    "* [使用 TorchScript 追踪模型](#torchscript)\n",
    "* [创建模型目录结构](#structure)\n",
    "* [创建配置文件](#configuration)\n",
    "* [在 Triton 推理服务器中加载模型](#load)\n",
    "* [将推理请求发送到服务器](#infer)\n",
    "* [练习](#exercise)\n",
    "* [小结](#conclusion)\n",
    "\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### 简介\n",
    "\n",
    "在此 notebook 中，我们将利用 HuggingFace 创建 PyTorch `XLMRobertaForSequenceClassification` 模型，使用 TorchScript 生成的代码将其写为本地 PyTorch 模型，然后使用 Triton 推理服务器加以部署。RoBERTa 是 BERT 模型架构的升级版。可以在[此处](https://huggingface.co/docs/transformers/model_doc/roberta)了解详情。我们的目标是了解 Triton 如何处理更复杂的模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structure\"></a>\n",
    "### 创建模型目录结构\n",
    "\n",
    "接下来，我们将创建模型目录结构。如需详细了解如何在 PyTorch 中创建模型目录结构，请参阅上一个 notebook [02_Simple_PyTorch_Model.ipynb](02_Simple_PyTorch_Model.ipynb)。\n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "├── huggingface-model\n",
    "│   ├── 1\n",
    "│   │   └── model.pt\n",
    "│   └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/huggingface-model\n",
    "!mkdir -p models/huggingface-model/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### 定义 HuggingFace 预训练模型\n",
    "\n",
    "在本节中，我们将导入文本分词和标记化函数，以创建 `XLMRobertaForSequenceClassification` 模型的输入文本的标记（即文本的数字化表示）。我们将封装我们要用的模型，将其设置为评估模式，并分配到 GPU 上。最后，我们将使用 `torch.jit.trace` 函数生成 TorchScript 代码，将虚构的输入传递给该代码，并将结果另存为 `model.pt` 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "\n",
    "\n",
    "R_tokenizer = XLMRobertaTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli')\n",
    "premise = \"Jupiter's Biggest Moons Started as Tiny Grains of Hail\"\n",
    "hypothesis = 'This text is about space & cosmos'\n",
    "\n",
    "input_ids = R_tokenizer.encode(premise, hypothesis, return_tensors='pt', \n",
    "                               max_length=256, padding='max_length')\n",
    "\n",
    "mask = input_ids != 1\n",
    "mask = mask.long()\n",
    "\n",
    "\n",
    "class PyTorch_to_TorchScript(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorch_to_TorchScript, self).__init__()\n",
    "        self.model = XLMRobertaForSequenceClassification.from_pretrained('joeddav/xlm-roberta-large-xnli', return_dict=False)\n",
    "    def forward(self, data, attention_mask=None):\n",
    "        return self.model(data.cuda(), attention_mask.cuda())\n",
    "\n",
    "pt_model = PyTorch_to_TorchScript().eval().cuda()\n",
    "# Jiyang: TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) \n",
    "#         that can then be run in a high-performance environment such as C++.\n",
    "# https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html\n",
    "traced_script_module = torch.jit.trace(pt_model, (input_ids, mask))\n",
    "traced_script_module.save('models/huggingface-model/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "### 创建配置文件\n",
    "\n",
    "接下来，我们将创建配置文件。如需详细了解如何在 PyTorch 中创建模型目录结构，请参阅上一个 notebook [02_Simple_PyTorch_Model.ipynb](02_Simple_PyTorch_Model.ipynb)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"huggingface-model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 1024\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  } ,\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/huggingface-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### 在 Triton 推理服务器中加载模型\n",
    "\n",
    "\n",
    "创建模型目录结构、定义和导出模型以及创建配置文件后，我们现在将等待 Triton 推理服务器来加载模型。我们设置此实验以在**轮询**模式下使用 Triton 推理服务器。这意味着 Triton 推理服务器将以 30 秒为间隔，持续轮询模型的修改内容或新创建的模型。请运行以下单元，为 Triton 推理服务器预留时间，以便在继续下一步操作前，对新模型/修改进行轮询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，我们的模型应已部署就绪且随时可用！为确认 Triton 推理服务器已启动并运行，我们会看到对以下 URL 的 `curl` 请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 Triton 已准备就绪，则 HTTP 请求将返回状态代码 200；如果尚未准备就绪，则返回 200 以外的状态代码。\n",
    "\n",
    "我们还可以向模型端点发送 `curl` 请求，以确认我们的模型已部署就绪并可随时使用。如果模型已准备就绪，则 `curl` 请求将返回状态代码 200；如果尚未准备就绪，则返回 200 以外的状态代码。\n",
    "\n",
    "此外，我们还将看到模型的相关信息，例如：\n",
    "\n",
    "* 模型的名称、\n",
    "* 模型可用的版本、\n",
    "* 后端平台（例如 pytorch_libtorch）\n",
    "* 附带各自名称、数据类型和形状的输入与输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/huggingface-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer\"></a>\n",
    "### 将推理请求发送到服务器\n",
    "\n",
    "HuggingFace 模型部署就绪后，即可向模型发送推理请求。\n",
    "\n",
    "首先，我们将开展一些内部维护并重启 Jupyter notebook 内核。此操作会释放部分 GPU 显存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将加载用于处理 NumPy 数据的 `tritonclient.http` 模组和实用函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将定义模型的输入和输出名称、模型名称、使用 Triton 推理服务器向其中部署模型的 URL（本例中为 `triton:8000` 的本地主机）以及模型版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['input__0', 'input__1']\n",
    "input_dtype = 'INT32'\n",
    "output_name = 'output__0'\n",
    "model_name = 'huggingface-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用 `tritonhttpclient.InferenceServerClient` 类通过 `.get_model_metadata()` 方法访问模型元数据，并使用 `get_model_config()` 方法获取模型配置，进而实例化客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将创建分词器，将分词器应用于前提和主题，并处理要传递给 Triton 推理服务器的结果数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "\n",
    "# instantiate our tokenizer\n",
    "R_tokenizer = XLMRobertaTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli')\n",
    "\n",
    "# create our premise and topic to be passed into the model\n",
    "premise = 'Jupiter’s Biggest Moons Started as Tiny Grains of Hail'\n",
    "topic = 'This text is about space & cosmos'\n",
    "\n",
    "# encode our inputs, convert to numpy arrays, create our mask, and do some reshaping\n",
    "input_ids = R_tokenizer.encode(premise, topic, max_length=256, truncation=True, padding='max_length')\n",
    "input_ids = np.array(input_ids, dtype=np.int32)\n",
    "mask = input_ids != 1\n",
    "mask = np.array(mask, dtype=np.int32)\n",
    "mask = mask.reshape(1, 256) \n",
    "input_ids = input_ids.reshape(1, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用预期输入名称、形状和数据类型来实例化输入数据的占位符。然后将输入数据设置为文本的 NumPy 数组表示形式。还需仅使用输出名称实例化输出数据的占位符。\n",
    "\n",
    "最后，我们将使用 `triton_client.infer()` 方法将输入提交至 Triton 推理服务器，指定模型名称、模型版本、输入和输出，并将结果转换为 NumPy 数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name[0], (1, 256), input_dtype)\n",
    "input0.set_data_from_numpy(input_ids, binary_data=False)\n",
    "input1 = tritonhttpclient.InferInput(input_name[1], (1, 256), input_dtype)\n",
    "input1.set_data_from_numpy(mask, binary_data=False)\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name,  binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, inputs=[input0, input1], outputs=[output])\n",
    "logits = response.as_numpy(output_name)\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，对数据进行后处理，忽略模型的 logits 向量中的 \"中立\"（第1维）分类结果，并取用\"蕴含\"（第2维） 的分类概率作为标签为真的概率。所需的全部操作如上所述！我们的模型识别出前提句实际上是关于空间和宇宙的！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = softmax(entail_contradiction_logits)\n",
    "true_prob = probs[:,1].item() * 100\n",
    "print(f'Probability that the label is true: {true_prob:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### 小结\n",
    "\n",
    "在此 notebook 中，我们介绍了如何利用 HuggingFace 创建 PyTorch XLMRobertaForSequenceClassification 模型，使用 TorchScript 生成的代码将其写为本地 PyTorch 模型，然后使用 Triton 推理服务器加以部署。\n",
    "\n",
    "我们建议您运行下面的单元进行清理。此操作将释放 GPU 显存，以供实验的其它部分使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf models/huggingface-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
