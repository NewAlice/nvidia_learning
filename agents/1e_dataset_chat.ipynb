{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83e83a2",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.cn/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>使用大语言模型（LLM）构建 AI 智能体</h1>\n",
    "<h2><b>练习 1:</b> 数据集聊天</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c0846",
   "metadata": {},
   "source": [
    "**欢迎回来！这是本课程的第一项练习，看看您能做些什么！**\n",
    "\n",
    "这个 Notebook 是“主讲”Notebook 后的一个实践练习。该系列练习基于一个共同的数据集，逐步扩展到更复杂的 LLM 交互中。\n",
    "\n",
    "在之前的 Notebook 中，我们实现了一个基本的多智能体系统来生成合成的多轮对话。虽然这种方法在生成人工对话方面很有用，但它在最终用户应用中的实用性不足。\n",
    "\n",
    "### **学习目标：**\n",
    "\n",
    "**在这个 Notebook 中，我们将：**\n",
    "\n",
    "- 构建一个与数据集交互的简单用户聊天机器人。\n",
    "- 解决处理过大数据集的问题，它们超出了模型的上下文窗口。\n",
    "- 开发一个总结工作流以有效地预处理数据。\n",
    "\n",
    "这个数据集将在整个课程中使用，所以第一步是使其能够进行简单的交互聊天。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a44312",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **第一部分：** 设置一个聊天机器人\n",
    "\n",
    "从之前的练习中，您可以指定一个简单的聊天机器人，通过一个简单的循环与用户进行交互。基于此，以下函数建立了一个简单的聊天机器人循环，用户可以与 AI 智能体进行交互。如果没有提供处理函数（链），它将默认使用一个未实现的生成器，输出占位消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc53823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def not_implemented_gen(state):\n",
    "    \"\"\"A placeholder generator that informs users the chain is not yet implemented.\"\"\"\n",
    "    message = \"Chain Not Implemented. Enter with no inputs or interrupt execution to exit.\"\n",
    "    for letter in message:\n",
    "        yield letter\n",
    "        sleep(0.005)\n",
    "\n",
    "def chat_with_chain(state={}, chain=not_implemented_gen):\n",
    "    \"\"\"\n",
    "    Interactive chat function that processes user input through a specified chain.\n",
    "    \n",
    "    Parameters:\n",
    "        state (dict): Maintains chat history and context.\n",
    "        chain (callable): Function to generate responses based on the chat history.\n",
    "    \"\"\"\n",
    "    assert isinstance(state, dict)\n",
    "    state[\"messages\"] = state.get(\"messages\", [])\n",
    "    while True:\n",
    "        try:\n",
    "            human_msg = input(\"\\n[Human]:\")\n",
    "            if not human_msg.strip(): break\n",
    "            agent_msg = \"\"\n",
    "            state[\"messages\"] += [(\"user\", human_msg)]\n",
    "            print(flush=True)\n",
    "            print(\"[Agent]: \", end=\"\", flush=True)\n",
    "            for token in getattr(chain, \"stream\", chain)(state):\n",
    "                agent_msg += token\n",
    "                print(getattr(token, \"content\", token), end=\"\", flush=True)\n",
    "            state[\"messages\"] += [(\"ai\", agent_msg)]\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"KeyboardInterrupt\")\n",
    "            break\n",
    "\n",
    "# Initialize chat with the placeholder generator\n",
    "chat_with_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bcbe5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "基于此，我们可以定义一个包括 LLM 的对话工作流、一个提示模板和一个初始状态。将提供提示、llm 和输出解析器，请将它们组合到您的 `chat_with_chain` 函数中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac251ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from functools import partial\n",
    "\n",
    "# Define an NVIDIA-backed LLM\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://nim-llm:8000/v1\")\n",
    "\n",
    "# Define a structured prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "     \"Assist users with their course-related queries using the provided context. \"\n",
    "     \"Do not reference the 'context' as 'context' explicitly.\"),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "# Construct the processing pipeline\n",
    "chat_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Initialize chatbot state\n",
    "state = {\n",
    "    \"messages\": [(\"ai\", \"Hello! I'm the NVIDIA DLI Chatbot! How can I help you?\")],\n",
    "    \"context\": \"\",  # Empty for now; will be updated later\n",
    "}\n",
    "\n",
    "# Wrap function to integrate AI response generation\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "\n",
    "# Start the chatbot with the AI pipeline\n",
    "chat(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf8ba0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **第二部分：** 引入一些上下文\n",
    "\n",
    "在本课程中，我们将开始使用来自 GTC 2025 会议的小数据集，每个都是独立的议题，内容不尽相同。这应该让您想起有机累积的数据池……部分原因是它确实是这样。数据可以在 [`gtc-data-2025.csv`](./gtc-data-2025.csv) 中找到，下面就直接加载成列表吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccbb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "filepath = \"gtc-data-2025.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Convert to JSON for structured processing\n",
    "raw_entries = json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "# Display the first few records\n",
    "raw_entries[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ed0ecb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "我们可以快速将这些数据处理成更自然的格式，然后尝试将它们连接起来，创建一个有效的“上下文字符串”供模型使用。自动创建上下文在实际工作流中非常常见，以增强 LLM，所以在这里做也没理由不这样做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21fba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify(entry, description_key='description'):\n",
    "    \"\"\"Formats workshop details into a human-readable string.\"\"\"\n",
    "    return (\n",
    "        f\"{entry.get('name')}\\n\"\n",
    "        f\"Presenters: {entry.get('instructors')}\\n\"\n",
    "        f\"Description: {entry.get(description_key)}\"\n",
    "    )\n",
    "\n",
    "# Convert dataset entries to structured text\n",
    "raw_blurbs = [\n",
    "    f\"[Session {i+1}]\\n{stringify(entry)}\" \n",
    "    for i, entry in enumerate(raw_entries)\n",
    "]\n",
    "\n",
    "# Construct full context string\n",
    "raw_context = \"The following workshops are slated to be presented at NVIDIA's GTC 2025 Conference:\\n\\n\"\n",
    "raw_context += \"\\n\\n\".join(raw_blurbs)\n",
    "\n",
    "# Display context statistics\n",
    "print(f\"Full Context Length (characters): {len(raw_context)}\")\n",
    "print(raw_context[:2000])  # Preview the first portion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8432f2d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**[TODO] 使用您之前的抽象，将上下文传递到提示词中，看看是否有效：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Initialize your state based on your opinionated chat chain\n",
    "state = {}\n",
    "\n",
    "try:\n",
    "    ## TODO: Perform the conversation with your long context (it's ok if it fails)\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ad3dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details><summary><b>提示</b></summary>\n",
    "\n",
    "记得有一个 `chat` 函数，它封装了可重用的工作流。所以可以使用 `chat(state)`来调用它。然后，只需要弄清楚的提示词中应该放什么。为了提醒您的记忆：\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "     \"Assist users with their course-related queries using the provided context. \"\n",
    "     \"Do not reference the 'context' as 'context' explicitly.\"),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "<details><summary><b>参考答案</b></summary>\n",
    "\n",
    "考虑到提示词期望接收一个包含 `context`（可解释为字符串）和 `messages`（可解释为消息列表，如 `[(\"user\", \"Hello World\")]`）的字典，我们可以以没有消息历史的状态开始，将 `raw_context` 作为上下文。\n",
    "\n",
    "```python\n",
    "state = {\"messages\": [], \"context\": raw_context,}\n",
    "try:\n",
    "    chat(state)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "这可能没有成功，原因也明显。模型的最大上下文为 `2^13 = 8192` 个 token，而当前上下文可能有点长。让我们用一个 [tokenizer](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct/blob/main/tokenizer.json) 来验证一下相关模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13557c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "llama_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\", clean_up_tokenization_spaces=True)\n",
    "\n",
    "def token_len(text):\n",
    "    return len(llama_tokenizer.encode(text=text))\n",
    "\n",
    "print(f\"String Length of Context: {len(raw_context)}\")\n",
    "print(f\"Token Length of Context: {token_len(raw_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed0d0d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**您可能会想“难道大多数模型的上下文不是更长吗？”您说的对，但有一些关键的注意事项：**\n",
    "\n",
    "- 如果使用 API 服务，您仍然会为这些 token 付费，所以长的静态上下文可能不是最好的主意？\n",
    "- 即使您的上下文长度是被支持的，输入越长，大多数模型的质量仍然会下降。此外，更多的输入意味着有更多相互矛盾的数据和文本结构的机会。\n",
    "- 对于任意文档甚至文档池，您可能会发现自己经常拓展到最大上下文。即使模型对这个数据集有效，它对数据库是否仍然适用呢？\n",
    "\n",
    "在我们的例子中，处理的是各种细节和质量的课程描述样本，总体而言，这形成了一个不一致的条目池。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5304f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sorted_raw_blurbs = sorted(raw_blurbs, key=token_len)\n",
    "\n",
    "def plot_token_len(entries, color=\"green\", alpha=1, len_fn=token_len):\n",
    "    plt.bar(x=range(len(entries)), height=[len_fn(v) for v in entries], width=1.0, color=color, alpha=alpha)\n",
    "\n",
    "plot_token_len(sorted_raw_blurbs, color=\"grey\")\n",
    "plt.xlabel(\"Entry Sample\")\n",
    "plt.ylabel(\"Token Length\")\n",
    "plt.show() \n",
    "\n",
    "print(\"SHORTEST ENTRIES:\")\n",
    "sample_blurbs = sorted_raw_blurbs[:5]\n",
    "# sample_blurbs = sorted_raw_blurbs[:5] + sorted_raw_blurbs[-3:]\n",
    "for entry in sample_blurbs:\n",
    "    print(entry, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a67c8",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **第三部分：** 总结长上下文\n",
    "\n",
    "也许可以把这些条目转换成更统一的形式？作为一个预处理步骤，可以将所有这些条目处理成一个更一致的形式。这不仅有助于模型推理整体上下文，还能利用条目的统一性来提高提示词的一致性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## TODO: Create a symmary system message to instruct the LLM.\n",
    "## Reuse the chat_chain as-it-was, remembering that it expects \"messages\" and \"context\"\n",
    "summary_msg = (\n",
    "    \"Summarize the presentation description down to only a few important sentences.\"\n",
    "    \" Start with '(Summary) '\"\n",
    "    ## Feel free to customize\n",
    ")\n",
    "\n",
    "def summarize(context_str, summary_msg=summary_msg):\n",
    "    return \"(Summary) No summary\"\n",
    "\n",
    "print(summarize(stringify(raw_entries[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317630e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details><summary><b>参考答案</b></summary>\n",
    "\n",
    "```python\n",
    "return chat_chain.invoke({\n",
    "    \"messages\": [(\"user\", summary_msg)],\n",
    "    \"context\": context_str\n",
    "})\n",
    "```\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "这是一种自然语言，不要求格式很好，但我们可以通过适当的提示工程为简单的文本到文本转换函数做好准备。还可以使用 LangChain 的批处理原语来大大简化并发管理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d70df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from tqdm.auto import tqdm\n",
    "import threading\n",
    "\n",
    "batch_inputs = [stringify(entry_dict) for entry_dict in raw_entries]\n",
    "\n",
    "## Simple version of a batched process. No progress bar\n",
    "# summaries = RunnableLambda(summarize).batch(batch_inputs, config={\"max_concurrency\": 20})\n",
    "\n",
    "## Modified version which also has progress bars! Marginally-slower, same backbone\n",
    "def batch_process(fn, inputs, max_concurrency=20):\n",
    "    lock = threading.Lock()\n",
    "    pbar = tqdm(total=len(inputs))\n",
    "    def process_doc(value):\n",
    "        try:\n",
    "            output = fn(value)\n",
    "        except Exception as e: \n",
    "            print(f\"Exception in thread: {e}\")\n",
    "        with lock:\n",
    "            pbar.update(1)\n",
    "        return output\n",
    "    try:\n",
    "        lc_runnable = fn if hasattr(fn, \"batch\") else RunnableLambda(process_doc)\n",
    "        return lc_runnable.batch(inputs, config={\"max_concurrency\": max_concurrency})\n",
    "    finally:\n",
    "        pbar.close()\n",
    "\n",
    "summaries = batch_process(summarize, batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9053d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "现在可以看看使用这个合成描述而不是原始描述会发生什么，同时考虑上下文长度是如何缩短的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d526f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stringify(entry, description_key='description'):\n",
    "#     return (\n",
    "#         f\"{entry.get('name')}\"\n",
    "#         f\"\\nPresentors: {entry.get('instructors')}\"\n",
    "#         f\"\\nDescription: {entry.get(description_key)}\"\n",
    "#     )\n",
    "\n",
    "for summary, pres_entry in zip(summaries, raw_entries):\n",
    "    words = summary.split()\n",
    "    if \"summary\" in words[0].lower():\n",
    "        words = words[1:]\n",
    "    pres_entry[\"summary\"] = \" \".join(words)\n",
    "\n",
    "print(stringify(raw_entries[0], \"summary\"))\n",
    "raw_entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "contexts_with_summaries = [stringify(entry, \"summary\") for entry in raw_entries]\n",
    "contexts_with_descripts = [stringify(entry) for entry in raw_entries]\n",
    "\n",
    "def plot_token_len(entries, color=\"green\", alpha=1, len_fn=token_len):\n",
    "    plt.bar(x=range(len(entries)), height=[len_fn(v) for v in entries], width=1.0, color=color, alpha=alpha)    \n",
    "\n",
    "sorted_summs = [v for _,v in sorted(zip((token_len(x) for x in contexts_with_summaries), contexts_with_summaries))]\n",
    "sorted_origs = [v for _,v in sorted(zip((token_len(x) for x in contexts_with_descripts), contexts_with_descripts))]\n",
    "aligned_summs = [v for _,v in sorted(zip((token_len(x) for x in contexts_with_descripts), contexts_with_summaries))]\n",
    "plot_token_len(sorted_origs, alpha=0.6, color=\"grey\")\n",
    "plot_token_len(sorted_summs, alpha=0.6, color=\"lightgreen\")\n",
    "plot_token_len(aligned_summs, alpha=0.6, color=\"green\")\n",
    "plt.xlabel(\"Entry Sample\")\n",
    "plt.ylabel(\"Token Length\")\n",
    "plt.show() \n",
    "\n",
    "print(\"Samples:\")\n",
    "sorted_raw_entries = sorted(raw_entries, key=(lambda v: token_len(str(v.get(\"description\")))))\n",
    "for entry in sorted_raw_entries[:3] + sorted_raw_entries[-3:]:\n",
    "    print(\n",
    "        f\"{entry.get('name')}\"\n",
    "        f\"\\nPresentors: {entry.get('instructors')}\"\n",
    "        f\"\\nDescription: {entry.get('description')}\"\n",
    "        f\"\\nSummary: {entry.get('summary')}\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8742b1a7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "看起来不错！让我们实现一下，并将这个更改应用到所有的条目上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify(entry, description_key='description'):\n",
    "    return (\n",
    "        f\"{entry.get('name')}\"\n",
    "        f\"\\nPresentors: {entry.get('instructors')}\"\n",
    "        f\"\\nDescription: {entry.get(description_key)}\"\n",
    "    )\n",
    "\n",
    "summary_blurbs = [f\"{stringify(entry, 'summary')}\" for i, entry in enumerate(raw_entries)]\n",
    "\n",
    "new_context = \"The following workshops are slated to be presented at NVIDIA's GTC 2025 Conference:\\n\\n\"\n",
    "new_context += \"\\n\\n\".join(summary_blurbs)\n",
    "print(\"New Context Length:\", len(new_context))\n",
    "print(f\"New Context Tokens: {token_len(new_context)}\")\n",
    "\n",
    "print(new_context[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43cdc89",
   "metadata": {},
   "source": [
    "输入大小的阈值突然就降低了！是时候替换上下文并进行测试了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db80097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "## Defined Earlier. Feel free to play around with this\n",
    "\n",
    "# Define an NVIDIA-backed LLM\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://nim-llm:8000/v1\")\n",
    "\n",
    "# Define a structured prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "     \"Assist users with their course-related queries using the provided context. \"\n",
    "     \"Do not reference the 'context' as 'context' explicitly.\"),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "# Construct the processing pipeline\n",
    "chat_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Initialize chatbot state\n",
    "state = {\n",
    "    \"messages\": [(\"ai\", \"Hello! I'm the NVIDIA DLI Chatbot! How can I help you?\")],\n",
    "    \"context\": \"\",  # Empty for now; will be updated later\n",
    "}\n",
    "\n",
    "# Wrap function to integrate AI response generation\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "\n",
    "## Defined Earlier\n",
    "#############################################################################\n",
    "\n",
    "state = {\n",
    "    \"messages\": [],\n",
    "    \"context\": new_context,\n",
    "}\n",
    "\n",
    "try:  ## HINT: Consider putting your call logic in the try-catch\n",
    "    chat(state)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consider saving the material as well, since it will be useful for later\n",
    "## For those who may take this course over multiple sessions, a version is provided.\n",
    "with open(\"simple_long_context.txt\", \"w\") as f:\n",
    "    f.write(new_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782be027",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **第四部分：** 反思这个练习\n",
    "\n",
    "这个练习其实很简单，从某种意义上来说，确实引导我们得到了一个有趣的系统。\n",
    "- 从表面看，我们所做的就是将过长的上下文转换成不过长的上下文。\n",
    "- 用其它术语来说，我们将全局环境中的元素“规范化”成一个合理的“标准上下文”，以便为主要 LLM 提供支持。\n",
    "- 悲观地看，我们通过将上下文*稍微缩短*到比最大输入长度更短，创造了一个短期内解决有限上下文空间的多轮问题的方案。\n",
    "- 乐观地看，现在有了一个可以复用的上下文，这有助于将整个上下文保持在模型的输入域内，适用于大多数单轮问题（包括可能补充多轮解决方案的问题）。\n",
    "\n",
    "在接下来的 Notebook 中，我们将继续使用这个练习的结果，希望随着了解的深入，这个简单过程的实用性会逐渐显现出来！"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
