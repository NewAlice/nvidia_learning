{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">异步流及 CUDA C/C++ 应用程序的可视化性能分析</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA工具包附带了 **Nsight Systems**，这是一个功能强大的GUI应用程序，可支持CUDA应用程序的开发。 Nsight Systems为被加速的应用程序生成图形化的活动时间表，其中包含有关CUDA API调用、内核执行、内存活动以及**CUDA流**的使用的详细信息。\n",
    "\n",
    "在本实验中，您将使用Nsight Systems时间表来指导您优化被加速的应用程序。此外，您还将学习一些中级的CUDA编程技术来支持您的工作：**手动的内存分配和迁移**； **固定**或**页面锁定**的主机内存；以及**非默认并发CUDA流**。\n",
    "\n",
    "本实验学习课程结束时，我们将为您提供一份评估测试，让您加速和优化一款简单的 n-body 模拟器，这可让您借此展示在本课程学习期间所掌握的技能。若您能够在保持正确性的同时加速模拟器，我们将为您颁发证书以资证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 预备知识\n",
    "\n",
    "为了充分利用本实验，您应该已经能够：\n",
    "\n",
    "- 编写，编译和运行既调用CPU函数又启动GPU核函数的C / C ++程序。\n",
    "- 使用执行配置(execution configuration)控制并行线程的层次结构。\n",
    "- 重构串行循环以在GPU上并行执行其迭代。\n",
    "- 分配和释放CUDA统一内存。\n",
    "- 了解统一内存在页面错误和数据迁移方面的行为。\n",
    "- 使用异步内存预取可以减少页面错误和数据迁移。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标\n",
    "\n",
    "在完成本练习后，您将能够：\n",
    "\n",
    "- 使用**Nsight Systems**直观地描述由GPU加速的CUDA应用程序的时间表。\n",
    "- 使用**Nsight Systems**识别和利用CUDA应用程序中的优化机会。\n",
    "- 利用CUDA流在被加速的应用程序中并发执行核函数。\n",
    "- （ **可选的进阶内容** ）使用手动的设备内存分配，包括分配固定的内存，以便在并发CUDA流之间异步传输数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 运行Nsight Systems\n",
    "\n",
    "在此交互式实验环境中，我们设置了一个远程桌面，您可以从您的浏览器访问该桌面，并在其中启动和使用Nsight Systems。\n",
    "\n",
    "首先，为一个已经存在的向量加法程序创建一个性能报告文件，然后逐步执行一系列步骤，以在Nsight Systems中打开该报告文件，并提供良好的视觉体验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成报告文件\n",
    "\n",
    "[`01-vector-add.cu`](../edit/01-vector-add/01-vector-add.cu)（<--单击这个链接以在浏览器中对源文件进行编辑）包含一个正常的矢量加法程序。 使用下方的代码执行单元（通过按CTRL+ENTER，您可以执行它以及本实验中的任何代码执行单元）来编译并运行它。 您应该看到一条表明已成功完成的消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-no-prefetch 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，使用`nsys profile --stats = true`创建一个报告文件，您将可以在Nsight Systems可视化分析器中打开该文件。 在这里，我们使用-o标志为报告文件起一个容易记住的文件名："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/vector-add-no-prefetch-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./vector-add-no-prefetch\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/vector-add-no-prefetch-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t9566 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/vector-add-no-prefetch-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/vector-add-no-prefetch-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 9524 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/vector-add-no-prefetch-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   57.6       211667355           3      70555785.0           21063       211601040  cudaMallocManaged                                                               \n",
      "   37.1       136414200           1     136414200.0       136414200       136414200  cudaDeviceSynchronize                                                           \n",
      "    5.2        19066916           3       6355638.7         5716127         7537268  cudaFree                                                                        \n",
      "    0.0           52966           1         52966.0           52966           52966  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0       136401810           1     136401810.0       136401810       136401810  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   78.8        78282848        7705         10160.0            1824          128000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   21.2        21121344         768         27501.8            1504          161920  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         393216.0            7705               51.0              4.000              760.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   54.4      1338229251          73      18331907.5           16802       100129918  poll                                                                            \n",
      "   41.3      1016037253          73      13918318.5            9011       100068577  sem_timedwait                                                                   \n",
      "    3.3        81586052         588        138751.8            1067        18576138  ioctl                                                                           \n",
      "    0.9        21001170          90        233346.3            1008         7476339  mmap                                                                            \n",
      "    0.0          552415          77          7174.2            2287           16622  open64                                                                          \n",
      "    0.0          121703           4         30425.8           22150           40449  pthread_create                                                                  \n",
      "    0.0           92223          23          4009.7            1212           13854  fopen                                                                           \n",
      "    0.0           81696           3         27232.0           19494           38238  fgets                                                                           \n",
      "    0.0           78094          11          7099.5            1923           14884  write                                                                           \n",
      "    0.0           38962          13          2997.1            1623            5282  munmap                                                                          \n",
      "    0.0           28037           5          5607.4            2675            8184  open                                                                            \n",
      "    0.0           27062          16          1691.4            1067            2622  fclose                                                                          \n",
      "    0.0           21146          10          2114.6            1177            3046  read                                                                            \n",
      "    0.0           11829           3          3943.0            3169            4445  pipe2                                                                           \n",
      "    0.0            8802           2          4401.0            4015            4787  socket                                                                          \n",
      "    0.0            6391           2          3195.5            3010            3381  fread                                                                           \n",
      "    0.0            6300           4          1575.0            1418            1727  mprotect                                                                        \n",
      "    0.0            5048           2          2524.0            1084            3964  fcntl                                                                           \n",
      "    0.0            4689           1          4689.0            4689            4689  connect                                                                         \n",
      "    0.0            2179           1          2179.0            2179            2179  bind                                                                            \n",
      "    0.0            1345           1          1345.0            1345            1345  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-no-prefetch-report ./vector-add-no-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开远程桌面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下一个代码单元格以生成指向远程桌面的URL，您应该通过复制该URL并将其粘贴到新的浏览器选项卡中来打开该URL。 然后，按照notebook里的指示进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = window.location.hostname + '/nsight/';\n",
       "element.append(url)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = window.location.hostname + '/nsight/';\n",
    "element.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单击 _Connect_ 按钮后，远程桌面将要求您输入密码，即`nvidia`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开远程桌面终端应用程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，单击位于远程桌面屏幕底部的命令行终端应用程序图标："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![terminal](images/terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要打开Nsight Systems，请从现在打开的命令行终端中输入并运行`nsight-sys`（注意没有空格）命令："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open nsight](images/open-nsight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启用使用情况报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现提示时，单击“Yes”以启用使用情况报告："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![enable usage](images/enable_usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开报告文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过从Nsight Systems菜单访问 _File_-> _Open_ 来打开此报告文件，然后转到路径 `/root/Desktop/reports`并选择`vector-add-no-prefetch-report.qdrep`。 您在本实验中生成的所有报告都将位于此`root/Desktop/reports`目录中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open-report](images/open-report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 忽略警告/错误信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以关闭并忽略看到的所有警告或错误，这仅仅是我们特定的远程桌面环境产生的结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ignore errors](images/ignore-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为活动时间表腾出更多的显示空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使您的体验更好，可全屏显示性能分析结果，即关闭 _Project Explorer_ 并隐藏 _Events View_ ："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![make nice](images/make-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "屏幕现在应如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![now nice](images/now-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展CUDA统一内存的活动时间表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，展开 _CUDA_-> _Unified memory_ 和 _Context_ 时间表，并关闭 _OS运行时库_ 的时间表："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open memory](images/open-memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察到有很多次的内存传输操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "略看一下即知，您的应用程序运行大约需要1秒钟，并且在运行`addVectorsInto`核函数期间，还有很多的UM内存活动："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![memory and kernel](images/memory-and-kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "放大内存的时间轴，可以更清楚地看到由按需内存页面错误引起的所有小内存传输。 几个提示：\n",
    "\n",
    "1. 您可以在滚动鼠标/触控板的同时按住`Ctrl`来放大和缩小时间轴的任何一点\n",
    "2. 您可以通过以下方式放大任意部分：单击+拖动它周围的矩形，然后选择 _Zoom in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是放大查看许多小的内存传输的示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![many transfers](images/many-transfers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 用Nsight Systems重复比较不同的代码重构后的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，您已经启动并运行了Nsight Systems，并且可以轻松地在活动时间表里移动位置。接下来，您将对您使用已经熟悉的技术进行了迭代改进的程序进行性能分析。 每做一次性能分析，活动时间表都会对下一步如何修改代码提供有用的信息。 这样做还将使您更深地理解各种CUDA编程技术如何影响应用程序的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：比较预取与不预取的活动时间表\n",
    "\n",
    "[`01-vector-add-prefetch-solution.cu`](../edit/01-vector-add/solutions/01-vector-add-prefetch-solution.cu)对上面的向量加法程序进行重构，以便它的`addVectorsInto`核函数所需的3个向量在核函数被调用之前就被异步预取到活动的GPU设备上（使用[`cudaMemPrefetchAsync`](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42)）。 请打开源代码，并确定这些更改在应用程序中的何处进行的。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印出来的成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-prefetch 01-vector-add/solutions/01-vector-add-prefetch-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/vector-add-prefetch-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./vector-add-prefetch\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/vector-add-prefetch-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t2060 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/vector-add-prefetch-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/vector-add-prefetch-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 2020 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/vector-add-prefetch-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   71.5       222314075           3      74104691.7           18876       222255071  cudaMallocManaged                                                               \n",
      "   19.6        61040368           1      61040368.0        61040368        61040368  cudaDeviceSynchronize                                                           \n",
      "    6.1        19002844           3       6334281.3         5394205         8063529  cudaFree                                                                        \n",
      "    2.8         8566265           3       2855421.7            7658         8447645  cudaMemPrefetchAsync                                                            \n",
      "    0.0           43486           1         43486.0           43486           43486  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0         1699935           1       1699935.0         1699935         1699935  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   75.7        65849792         192        342967.7          339840          346592  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   24.3        21127936         768         27510.3            1600          160576  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         393216.0             192             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   51.3      1184748329          70      16924976.1           18278       100127027  poll                                                                            \n",
      "   40.9       942777135          66      14284502.0           11796       100069013  sem_timedwait                                                                   \n",
      "    5.6       129725432         591        219501.6            1002        22396463  ioctl                                                                           \n",
      "    1.2        28249309           3       9416436.3           36201        14141462  sem_wait                                                                        \n",
      "    0.9        20938555          90        232650.6            1101         8003323  mmap                                                                            \n",
      "    0.0          576045          77          7481.1            2669           15297  open64                                                                          \n",
      "    0.0          147706           5         29541.2           21780           39538  pthread_create                                                                  \n",
      "    0.0           95675          13          7359.6            3571           13476  write                                                                           \n",
      "    0.0           94764          23          4120.2            1328           14642  fopen                                                                           \n",
      "    0.0           82683           3         27561.0           21777           37883  fgets                                                                           \n",
      "    0.0           41077          14          2934.1            1465            4268  munmap                                                                          \n",
      "    0.0           31152          15          2076.8            1080            3040  read                                                                            \n",
      "    0.0           28710           5          5742.0            2676            8265  open                                                                            \n",
      "    0.0           27611          16          1725.7            1066            3348  fclose                                                                          \n",
      "    0.0           12591           3          4197.0            3231            5408  pipe2                                                                           \n",
      "    0.0            9770           2          4885.0            3811            5959  socket                                                                          \n",
      "    0.0            8593           5          1718.6            1459            2229  mprotect                                                                        \n",
      "    0.0            6921           2          3460.5            2755            4166  fread                                                                           \n",
      "    0.0            5429           1          5429.0            5429            5429  connect                                                                         \n",
      "    0.0            4967           2          2483.5            1118            3849  fcntl                                                                           \n",
      "    0.0            1898           1          1898.0            1898            1898  bind                                                                            \n",
      "    0.0            1276           1          1276.0            1276            1276  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-prefetch-report ./vector-add-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开报告，让先前的报告保持打开的状态以进行比较。\n",
    "\n",
    "- 在添加异步预取之前，执行时间与`addVectorsInto`核函数的执行时间相比如何？\n",
    "- 在活动时间表的 *CUDA API* 部分中找到`cudaMemPrefetchAsync`。\n",
    "- 内存传输有什么变化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：使用核函数进行向量初始化并分析其性能\n",
    "\n",
    "在向量加法程序的先前迭代中，向量数据是在CPU上初始化的，因此需要在addVectorsInto核函数对其进行操作之前将其迁移到GPU上。\n",
    "\n",
    "该应用程序的下一个迭代[01-init-kernel-solution.cu](../edit/02-init-kernel/solutions/01-init-kernel-solution.cu)已重构为在GPU上并行处理初始化数据。\n",
    "\n",
    "由于初始化现是在GPU上进行的，因此预取要在初始化之前完成的，而不是在向量加法之前完成的。 查看源代码以识别在何处进行了这些更改。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-kernel 02-init-kernel/solutions/01-init-kernel-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/init-kernel-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./init-kernel\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/init-kernel-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1786 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/init-kernel-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/init-kernel-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1749 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/init-kernel-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   89.9       207876039           3      69292013.0           30881       207779206  cudaMallocManaged                                                               \n",
      "    7.2        16541061           3       5513687.0          801016        14794882  cudaFree                                                                        \n",
      "    1.5         3552258           1       3552258.0         3552258         3552258  cudaDeviceSynchronize                                                           \n",
      "    1.4         3147061           3       1049020.3         1002258         1124914  cudaMemPrefetchAsync                                                            \n",
      "    0.0           76361           4         19090.3            8252           45829  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.0         1860419           3        620139.7          611073          625313  initWith                                                                        \n",
      "   48.0         1715011           1       1715011.0         1715011         1715011  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21121536         768         27502.0            1600          160160  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   45.4       466384982          30      15546166.1            8865       100060771  sem_timedwait                                                                   \n",
      "   44.9       461383538          30      15379451.3           15931       100126475  poll                                                                            \n",
      "    7.9        80832822         590        137004.8            1009        15594514  ioctl                                                                           \n",
      "    1.8        18503694          90        205596.6            1072        14722124  mmap                                                                            \n",
      "    0.1          565264          77          7341.1            2299           13021  open64                                                                          \n",
      "    0.0          113845           4         28461.3           24632           32659  pthread_create                                                                  \n",
      "    0.0           92760          23          4033.0            1421           13160  fopen                                                                           \n",
      "    0.0           84674          11          7697.6            4246           12726  write                                                                           \n",
      "    0.0           82414           3         27471.3           19603           38413  fgets                                                                           \n",
      "    0.0           45330          15          3022.0            1587            4607  munmap                                                                          \n",
      "    0.0           27544          16          1721.5            1049            3263  fclose                                                                          \n",
      "    0.0           27316           5          5463.2            2700            8458  open                                                                            \n",
      "    0.0           24803          12          2066.9            1046            3077  read                                                                            \n",
      "    0.0           11417           3          3805.7            3585            3917  pipe2                                                                           \n",
      "    0.0           10053           2          5026.5            4974            5079  socket                                                                          \n",
      "    0.0            7739           3          2579.7            1272            3540  fread                                                                           \n",
      "    0.0            6271           4          1567.8            1431            1734  mprotect                                                                        \n",
      "    0.0            5355           1          5355.0            5355            5355  connect                                                                         \n",
      "    0.0            4890           2          2445.0            1013            3877  fcntl                                                                           \n",
      "    0.0            1869           1          1869.0            1869            1869  bind                                                                            \n",
      "    0.0            1344           1          1344.0            1344            1344  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-kernel-report ./init-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开新的报告文件，然后执行以下操作：\n",
    "\n",
    "- 将应用程序和`addVectorsInto`运行时间与应用程序的先前版本进行比较，有什么变化？\n",
    "- 查看时间表中的 *Kernels* 部分。 这两个核函数（`addVectorsInto`和初始化核函数）中的哪一个占用了GPU的大部分时间？\n",
    "- 您的应用程序包含以下哪项？\n",
    "   - 数据迁移（HtoD）\n",
    "   - 数据迁移（DtoH）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：使用异步预将数据取回主机，并分析其性能\n",
    "\n",
    "目前，向量加法应用程序是在主机上验证向量加法核函数的结果，所以在该应用程序的下一个重构[01-prefetch-check-solution.cu](../edit/04-prefetch-check/solutions/01-prefetch-check-solution.cu)中，我们用异步预取将数据取回主机后再进行验证。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印出来的成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-host 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/prefetch-to-host-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./prefetch-to-host\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/prefetch-to-host-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1079 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/prefetch-to-host-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/prefetch-to-host-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1044 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/prefetch-to-host-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   85.0       224328314           3      74776104.7           19567       224259286  cudaMallocManaged                                                               \n",
      "    9.5        25083747           4       6270936.8           10123        24256404  cudaMemPrefetchAsync                                                            \n",
      "    3.3         8623806           3       2874602.0          798599         6915204  cudaFree                                                                        \n",
      "    2.2         5790322           1       5790322.0         5790322         5790322  cudaDeviceSynchronize                                                           \n",
      "    0.0           77411           4         19352.8            9104           42274  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.3         1878559           3        626186.3          625440          626783  initWith                                                                        \n",
      "   47.7         1716544           1       1716544.0         1716544         1716544  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        20336000          64        317750.0          310944          319808  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.5       428684644          25      17147385.8            9019       100064475  sem_timedwait                                                                   \n",
      "   42.9       422213288          28      15079046.0           15602       100121438  poll                                                                            \n",
      "   12.4       121789717         590        206423.2            1032        24215905  ioctl                                                                           \n",
      "    1.1        10603014          91        116516.6            1001         6862504  mmap                                                                            \n",
      "    0.1          564855          77          7335.8            2496           14444  open64                                                                          \n",
      "    0.0          136610           5         27322.0           22738           30704  pthread_create                                                                  \n",
      "    0.0           95691          23          4160.5            1159           17011  fopen                                                                           \n",
      "    0.0           87676          12          7306.3            4169           12390  write                                                                           \n",
      "    0.0           84504           2         42252.0           18154           66350  sem_wait                                                                        \n",
      "    0.0           82425           3         27475.0           20806           37895  fgets                                                                           \n",
      "    0.0           36576          14          2612.6            1303            3770  munmap                                                                          \n",
      "    0.0           28241           5          5648.2            2654            8240  open                                                                            \n",
      "    0.0           26636          16          1664.8            1020            3196  fclose                                                                          \n",
      "    0.0           26484          13          2037.2            1000            2914  read                                                                            \n",
      "    0.0           11511           3          3837.0            3736            3899  pipe2                                                                           \n",
      "    0.0            9014           2          4507.0            3667            5347  socket                                                                          \n",
      "    0.0            7504           5          1500.8            1346            1612  mprotect                                                                        \n",
      "    0.0            7325           3          2441.7            1353            3424  fread                                                                           \n",
      "    0.0            5126           1          5126.0            5126            5126  connect                                                                         \n",
      "    0.0            3874           1          3874.0            3874            3874  fcntl                                                                           \n",
      "    0.0            1898           1          1898.0            1898            1898  bind                                                                            \n",
      "    0.0            1323           1          1323.0            1323            1323  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o prefetch-to-host-report ./prefetch-to-host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开此报告文件，然后执行以下操作：\n",
    "\n",
    "- 使用时间表里的 *Unified Memory*部分，对比向CPU添加预取之前和之后的*Data Migration (DtoH)*事件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 并发CUDA流\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您现在将学习一个新概念 **CUDA Streams**。 在对它们进行介绍之后，您将返回使用Nsight Systems来更好地评估它们对应用程序性能的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下幻灯片以可视化的方式概要地介绍了即将学习的内容。请在进入以下更详细的内容之前浏览每一页幻灯片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-1-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 CUDA 编程中，**流**是由按顺序执行的一系列命令构成。在 CUDA 应用程序中，核函数的执行以及一些内存传输均在 CUDA 流中进行。不过直至此时，您仍未直接与 CUDA 流打交道；但实际上您的 CUDA 代码已在名为*默认流*的流中执行了其核函数。\n",
    "\n",
    "除默认流以外，CUDA 程序员还可创建并使用非默认 CUDA 流，此举可支持执行多个操作，例如在不同的流中并发执行多个核函数。多流的使用可以为您的加速应用程序带来另外一个层次的并行，并能提供更多应用程序的优化机会。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 控制CUDA流行为的规则\n",
    "\n",
    "为有效利用 CUDA 流，您应了解有关 CUDA 流行为的以下几项规则：\n",
    "\n",
    "- 给定流中的所有操作会按序执行。\n",
    "- 就不同非默认流中的操作而言，无法保证其会按彼此之间的任何特定顺序执行。\n",
    "- 默认流具有阻断能力，即，它会等待其它已在运行的所有流完成当前操作之后才运行，但在其自身运行完毕之前亦会阻碍其它流的运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建，使用和销毁非默认CUDA流\n",
    "\n",
    "以下代码段演示了如何创建，利用和销毁非默认CUDA流。您会注意到，要在非默认CUDA流中启动CUDA核函数，必须将流作为执行配置的第4个可选参数传递给该核函数。到目前为止，您仅利用了执行配置的前两个参数：\n",
    "\n",
    "``` C++\n",
    "cudaStream_t stream;   // CUDA流的类型为 `cudaStream_t`\n",
    "cudaStreamCreate(&stream); // 注意，必须将一个指针传递给 `cudaCreateStream`\n",
    "\n",
    "someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>();   // `stream` 作为第4个EC参数传递\n",
    "\n",
    "cudaStreamDestroy(stream); // 注意，将值（而不是指针）传递给 `cudaDestroyStream`\n",
    "```\n",
    "\n",
    "但值得一提的是，执行配置的第3个可选参数超出了本实验的范围。此参数允许程序员提供**共享内存**（当前将不涉及的高级主题）中为每个内核启动动态分配的字节数。每个块分配给共享内存的默认字节数为“0”，在本练习的其余部分中，您将传递“ 0”作为该值，以便展示我们感兴趣的第4个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：预测默认流行为\n",
    "\n",
    "[01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) 应用程序带有一个非常简单的 `printNumber` 核函数，可用于接受及打印整数。仅在单个线程块内使用单线程执行该核函数，但使用“for 循环”可执行 5 次，并且每次启动时都会传递“for 循环”的迭代次数。\n",
    "\n",
    "使用下方的代码执行线程块编译和运行 [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) 应用程序。您应能看到打印的数字 `0` 至 `4`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既已了解核函数在默认情况下会在默认流中执行，那么据您预计，`print-numbers` 程序的 5 次启动将会顺次执行还是会并行执行？您应能提及默认流的两个功能来支持您的回答。在下面的单元格中创建一个报告文件，然后在Nsight Systems中将其打开以确认您的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/print-numbers-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./print-numbers\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u00000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\tGenerating the /dli/task/print-numbers-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t943 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/print-numbers-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/print-numbers-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 909 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/print-numbers-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.9       207349057           5      41469811.4            4819       207324122  cudaLaunchKernel                                                                \n",
      "    0.1          270582           1        270582.0          270582          270582  cudaDeviceSynchronize                                                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          283392           5         56678.4           54816           63392  printNumber                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.8       240083163          14      17148797.4           11470       100058263  sem_timedwait                                                                   \n",
      "   37.7       206574107          14      14755293.4            2002       100117770  poll                                                                            \n",
      "   17.9        98121751         574        170943.8            1072        20654721  ioctl                                                                           \n",
      "    0.4         2229747          81         27527.7            1108          719757  mmap                                                                            \n",
      "    0.1          571074          77          7416.5            3084           14972  open64                                                                          \n",
      "    0.0          113812           4         28453.0           22877           35788  pthread_create                                                                  \n",
      "    0.0           97335          23          4232.0            1183           16233  fopen                                                                           \n",
      "    0.0           92262          12          7688.5            4031           12124  write                                                                           \n",
      "    0.0           84226           3         28075.3           22765           38439  fgets                                                                           \n",
      "    0.0           28509          16          1781.8            1069            4247  fclose                                                                          \n",
      "    0.0           28140           5          5628.0            2947            8270  open                                                                            \n",
      "    0.0           25171          11          2288.3            1080            4539  read                                                                            \n",
      "    0.0           20305           7          2900.7            1237            4166  munmap                                                                          \n",
      "    0.0           11336           3          3778.7            3438            4116  pipe2                                                                           \n",
      "    0.0            8359           2          4179.5            3804            4555  socket                                                                          \n",
      "    0.0            6600           2          3300.0            3097            3503  fread                                                                           \n",
      "    0.0            6467           4          1616.8            1433            1784  mprotect                                                                        \n",
      "    0.0            5580           2          2790.0            1043            4537  fcntl                                                                           \n",
      "    0.0            4785           1          4785.0            4785            4785  connect                                                                         \n",
      "    0.0            2932           1          2932.0            2932            2932  fflush                                                                          \n",
      "    0.0            1862           1          1862.0            1862            1862  bind                                                                            \n",
      "    0.0            1354           1          1354.0            1354            1354  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-report ./print-numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：实现并发CUDA流\n",
    "\n",
    "由于核函数的所有 5 次启动均在同一个流中发生，因此看到 5 个核函数顺次执行也就不足为奇。此外，也可以这么说，由于默认流具有阻断作用，所以核函数都会在完成本次启动之后才启动下一次，而事实也是如此。\n",
    "\n",
    "请重构 [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) 应用程序，以便核函数的每次启动都在自身非默认流中进行。若已不再需要所创建的流，请务必予以销毁。请使用下方的代码执行单元编译和运行经重构的代码。您应仍能看到打印的数字 `0` 至 `4`，不过这些数字不一定会按升序排列。如您遇到问题，请参阅 [解决方案](../edit/05-stream-intro/solutions/01-print-numbers-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers-in-streams 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您既已为核函数的 5 次启动使用 5 个不同的非默认流，您预计它们会顺次执行还是会并行执行？除目前对流的了解之外，您还需考虑 `printNumber` 核函数的简单程度，也就是说，即使您预测并行运行，核函数的完成速度是否仍会允许完全重叠？\n",
    "\n",
    "进行假设后，在Nsight Systems中打开一个新的报告文件以查看其实际行为。 您应该注意到，现在，_CUDA_ 部分中为您创建的每个非默认流提供了其他行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/print-numbers-in-streams-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = print-numbers-in-streams\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\tGenerating the /dli/task/print-numbers-in-streams-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t952 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/print-numbers-in-streams-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/print-numbers-in-streams-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 917 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/print-numbers-in-streams-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.9       238033575           5      47606715.0            4924       238007978  cudaLaunchKernel                                                                \n",
      "    0.1          270407           1        270407.0          270407          270407  cudaDeviceSynchronize                                                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          283520           5         56704.0           54784           63264  printNumber                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   44.3       279716868          14      19979776.3           11430       100060104  sem_timedwait                                                                   \n",
      "   37.5       237110151          14      16936439.4            1305        82102439  poll                                                                            \n",
      "   17.5       110464482         580        190456.0            1019        19503656  ioctl                                                                           \n",
      "    0.5         3069538          82         37433.4            1351         1100962  mmap                                                                            \n",
      "    0.1          765612          77          9943.0            3644           20998  open64                                                                          \n",
      "    0.0          135614           4         33903.5           28748           36932  pthread_create                                                                  \n",
      "    0.0          130724          23          5683.7            1658           17265  fopen                                                                           \n",
      "    0.0          124783           3         41594.3           34830           54912  fgets                                                                           \n",
      "    0.0          124498          12         10374.8            5242           21563  write                                                                           \n",
      "    0.0           44634          16          2789.6            1169           11167  fclose                                                                          \n",
      "    0.0           39670           5          7934.0            4074           11690  open                                                                            \n",
      "    0.0           29904          12          2492.0            1018            3992  read                                                                            \n",
      "    0.0           23591           7          3370.1            1271            4138  munmap                                                                          \n",
      "    0.0           16355           3          5451.7            4942            6337  pipe2                                                                           \n",
      "    0.0           12892           2          6446.0            5633            7259  socket                                                                          \n",
      "    0.0            7580           4          1895.0            1815            2037  mprotect                                                                        \n",
      "    0.0            7163           1          7163.0            7163            7163  connect                                                                         \n",
      "    0.0            6208           2          3104.0            2696            3512  fread                                                                           \n",
      "    0.0            5895           2          2947.5            1209            4686  fcntl                                                                           \n",
      "    0.0            2931           1          2931.0            2931            2931  bind                                                                            \n",
      "    0.0            2869           1          2869.0            2869            2869  fflush                                                                          \n",
      "    0.0            1820           1          1820.0            1820            1820  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-in-streams-report print-numbers-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stream print](images/streams-print.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：将流用于并行进行数据初始化的核函数\n",
    "\n",
    "您一直使用的向量加法应用程序 [01-prefetch-check-solution.cu](../edit/04-prefetch-check/solutions/01-prefetch-check-solution.cu) 目前启动 3 次初始化核函数，即：为 `vectorAdd` 核函数需要初始化的 3 个向量分别启动一次。重构该应用程序，以便在其各自的非默认流中启动全部 3 个初始化核函数。在使用下方的代码执行单元编译及运行时，您应仍能看到打印的成功消息。如您遇到问题，请参阅 [解决方案](../edit/06-stream-init/solutions/01-stream-init-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-in-streams 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开一个报告，以确认您的3个初始化内核启动正在它们自己的非默认流中运行，并且存在一定程度的并发重叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/init-in-streams-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./init-in-streams\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/init-in-streams-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1072 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/init-in-streams-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/init-in-streams-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1037 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/init-in-streams-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   82.5       212816558           3      70938852.7           30098       212716965  cudaMallocManaged                                                               \n",
      "   12.8        32969378           4       8242344.5          962229        29921075  cudaMemPrefetchAsync                                                            \n",
      "    3.3         8617415           3       2872471.7          811050         6880060  cudaFree                                                                        \n",
      "    1.4         3553487           1       3553487.0         3553487         3553487  cudaDeviceSynchronize                                                           \n",
      "    0.0           79401           4         19850.3            7807           49155  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.2         1867424           3        622474.7          617152          627488  initWith                                                                        \n",
      "   47.8         1709407           1       1709407.0         1709407         1709407  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        20337120          64        317767.5          310912          320192  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   44.1       443347754          25      17733910.2            9576       100057886  sem_timedwait                                                                   \n",
      "   43.3       434651086          27      16098188.4           16009       100122082  poll                                                                            \n",
      "   11.4       114226793         591        193277.1            1111        29841554  ioctl                                                                           \n",
      "    1.1        10681091          90        118678.8            1124         6825546  mmap                                                                            \n",
      "    0.1          603804          77          7841.6            2132           18964  open64                                                                          \n",
      "    0.0          464239          11         42203.5            3753          397471  write                                                                           \n",
      "    0.0          111846           4         27961.5           21854           37955  pthread_create                                                                  \n",
      "    0.0          101233          23          4401.4            1206           13279  fopen                                                                           \n",
      "    0.0           83583           3         27861.0           22388           37982  fgets                                                                           \n",
      "    0.0           39532          14          2823.7            1058            4178  munmap                                                                          \n",
      "    0.0           29063           5          5812.6            2790            8629  open                                                                            \n",
      "    0.0           27916          16          1744.8            1036            3753  fclose                                                                          \n",
      "    0.0           24389          12          2032.4            1025            3240  read                                                                            \n",
      "    0.0           10634           3          3544.7            3073            3792  pipe2                                                                           \n",
      "    0.0           10365           2          5182.5            4270            6095  socket                                                                          \n",
      "    0.0            7201           3          2400.3            1342            3296  fread                                                                           \n",
      "    0.0            6093           4          1523.3            1266            1745  mprotect                                                                        \n",
      "    0.0            5704           1          5704.0            5704            5704  connect                                                                         \n",
      "    0.0            3720           1          3720.0            3720            3720  fcntl                                                                           \n",
      "    0.0            2310           1          2310.0            2310            2310  bind                                                                            \n",
      "    0.0            1403           1          1403.0            1403            1403  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-in-streams-report ./init-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 总结\n",
    "\n",
    "在本实验中，您学习了：\n",
    "\n",
    "- 使用 **Nsight Systems** 直观地描述GPU加速的CUDA应用程序的活动时间表。\n",
    "- 使用Nsight Systems识别和利用GPU加速的CUDA应用程序中的优化机会。\n",
    "- 利用CUDA流在被加速的应用程序中并发执行核函数。\n",
    "\n",
    "此时，您拥有大量的基本工具和技术，可用于加速原本只能在CPU上的应用程序，然后不断优化那些被加速的应用程序。 在最后的练习中，您将有机会应用学到的所有知识来加速[n-body](https://en.wikipedia.org/wiki/N-body_problem)模拟器，以预测受引力互相影响的一组物体的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 最后的练习：加速和优化N体模拟器\n",
    "\n",
    "[n-body](https://en.wikipedia.org/wiki/N-body_problem) 模拟器可以预测通过引力相互作用的一组物体的个体运动。[01-nbody.cu](../edit/09-nbody/01-nbody.cu) 包含一个简单而有效的 n-body 模拟器，适合用于在三维空间移动的物体。我们可通过向该应用程序传递一个命令行参数以影响系统中的物体数量。\n",
    "\n",
    "以目前的仅用CPU的情况下，此应用程序大约需要5秒钟才能运行4096个物体，需要**20分钟**才能运行65536个物体。您的任务是用GPU加速程序，同时保持仿真的正确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 做此练习时可考虑的注意事项\n",
    "\n",
    "在开始任务之前，请注意以下事项：\n",
    "\n",
    "- 在第一次重构中，请格外注意应用程序的逻辑部分（尤其是 `bodyForce` 函数）能够并且应该基本保持不变：要侧重于尽可能轻松地加速应用程序。\n",
    "- 代码库包含 `main` 函数内的“for 循环”，用于将 `bodyForce` 函数计算的物体间的引力集成到系统中每个物体的所在位置。该集成不仅需在 `bodyForce` 函数运行后进行，并且需在下一次调用 `bodyForce` 函数之前完成。在选择并行化的处理方式和程序位置时，请牢记这一点。\n",
    "- 使用基于性能分析驱动和迭代的方法。\n",
    "- 无需为代码添加错误处理，但其可能有助您确保代码的顺利运行。\n",
    "\n",
    "快去开心地执行任务吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用此单元格来编译nbody模拟器。尽管它最初只是一个仅用于CPU的应用程序，但却可以准确地模拟物体的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++11 -o nbody 09-nbody/01-nbody.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强烈建议您使用性能分析器来协助您的工作。执行以下单元格以生成性能分析报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/nbody-report\n",
      "\tforce-overwrite = true\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./nbody\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "\n",
      "The target application returned non-zero exit code 11\n",
      "\n",
      "\tGenerating the /dli/task/nbody-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t926 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/nbody-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/nbody-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 900 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/nbody-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CUDA trace data was not collected.\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.5       310805905          15      20720393.7            1707       100121654  poll                                                                            \n",
      "   42.2       300984918          13      23152686.0           10119       100175820  sem_timedwait                                                                   \n",
      "   13.8        98403271         574        171434.3            1030        18954405  ioctl                                                                           \n",
      "    0.4         2577351          82         31431.1            1150          703444  mmap                                                                            \n",
      "    0.1          637818          77          8283.4            4410           17144  open64                                                                          \n",
      "    0.0          141040          25          5641.6            1334           17433  fopen                                                                           \n",
      "    0.0          110558           4         27639.5           22831           33043  pthread_create                                                                  \n",
      "    0.0           85805          11          7800.5            4266           14305  write                                                                           \n",
      "    0.0           82312           3         27437.3           19907           38062  fgets                                                                           \n",
      "    0.0           37949          17          2232.3            1072            8461  fclose                                                                          \n",
      "    0.0           29810           5          5962.0            4310            8095  open                                                                            \n",
      "    0.0           25572           8          3196.5            1255            5199  munmap                                                                          \n",
      "    0.0           23467          10          2346.7            1051            4038  read                                                                            \n",
      "    0.0           18997           3          6332.3            3606           10479  fwrite                                                                          \n",
      "    0.0           12227           3          4075.7            3219            5461  pipe2                                                                           \n",
      "    0.0           10186           2          5093.0            4915            5271  socket                                                                          \n",
      "    0.0            9657           3          3219.0            1433            4561  fcntl                                                                           \n",
      "    0.0            6491           2          3245.5            2889            3602  fread                                                                           \n",
      "    0.0            6294           4          1573.5            1531            1619  mprotect                                                                        \n",
      "    0.0            5921           1          5921.0            5921            5921  connect                                                                         \n",
      "    0.0            4687           1          4687.0            4687            4687  fflush                                                                          \n",
      "    0.0            1963           1          1963.0            1963            1963  bind                                                                            \n",
      "    0.0            1531           1          1531.0            1531            1531  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true --force-overwrite=true -o nbody-report ./nbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们导入一个函数，该函数将针对各种数量的物体运行您的`nbody`模拟器，以检查其性能和准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assessment import run_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行以下单元格以运行并评估`nbody`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用4096个物体运行n-体模拟器\n",
      "----------------------------------------\n",
      "\n",
      "此应用程序应该运行得快于0.9秒。\n",
      "您的应用程序运行了: 0.2686秒\n",
      "您的应用程序运行速度是  \n",
      "您的结果是正确的。\n",
      "\n",
      "使用65536个物体运行n-体模拟器\n",
      "----------------------------------------\n",
      "\n",
      "此应用程序应该运行得快于1.3秒。\n",
      "您的应用程序运行了: 0.6128秒\n",
      "您的应用程序运行速度是  \n",
      "您的结果是正确的。\n",
      "\n",
      "祝贺您！您通过了评估！\n",
      "请参阅下面的说明来生成证书，并查看您是否可以进一步加速模拟器！\n"
     ]
    }
   ],
   "source": [
    "run_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成证书"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您通过了评估，请返回课程页面（如下所示）并单击\"ASSESS TASK\"（评估任务）按钮，这将为您生成该课程的证书。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run_assessment](./images/run_assessment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进阶内容\n",
    "\n",
    "以下章节专为时间富余和有意深究的学习者而设，其中将介绍更多中级技术，其中会涉及部分手动内存管理，以及使用非默认流重叠执行核函数和内存拷贝。\n",
    "\n",
    "在了解以下所列的各项技术后，您可尝试运用这些技术进一步优化 n-body 模拟器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 手动内存分配和复制\n",
    "\n",
    "尽管 `cudaMallocManaged` 和 `cudaMemPrefetchAsync` 函数性能出众并能大幅简化内存迁移，但有时也有必要使用更多手动内存分配方法。这在已知只需在设备或主机上访问数据时尤其如此，并且因免于进行自动按需迁移而能够收回数据迁移成本。\n",
    "\n",
    "此外，通过手动内存管理，您可以使用非默认流同时开展数据传输与计算工作。在本节中，您将学习一些基本的手动内存分配和拷贝技术，之后会延伸应用这些技术以同时开展数据拷贝与计算工作。\n",
    "\n",
    "以下是一些用于手动内存管理的 CUDA 命令：\n",
    "\n",
    "- `cudaMalloc` 命令将直接为处于活动状态的 GPU 分配内存。这可防止出现所有 GPU 分页错误，而代价是主机代码将无法访问该命令返回的指针。\n",
    "- `cudaMallocHost` 命令将直接为 CPU 分配内存。该命令可“固定”内存(pinned memory)或“页锁定”内存(page-locked memory)，此举允许将内存异步拷贝至 GPU 或从 GPU 异步拷贝至内存。固定内存过多则会干扰 CPU 性能，因此请勿无端使用该命令。释放固定内存时应使用 `cudaFreeHost` 命令。\n",
    "- 无论是从主机到设备还是从设备到主机，`cudaMemcpy` 命令均可拷贝（而非传输）内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动内存管理示例\n",
    "\n",
    "以下是一段演示使用上述 CUDA API 调用的代码。\n",
    "\n",
    "```cpp\n",
    "int *host_a, *device_a;        // Define host-specific and device-specific arrays.\n",
    "cudaMalloc(&device_a, size);   // `device_a` is immediately available on the GPU.\n",
    "cudaMallocHost(&host_a, size); // `host_a` is immediately available on CPU, and is page-locked, or pinned.\n",
    "\n",
    "initializeOnHost(host_a, N);   // No CPU page faulting since memory is already allocated on the host.\n",
    "\n",
    "// `cudaMemcpy` takes the destination, source, size, and a CUDA-provided variable for the direction of the copy.\n",
    "cudaMemcpy(device_a, host_a, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "kernel<<<blocks, threads, 0, someStream>>>(device_a, N);\n",
    "\n",
    "// `cudaMemcpy` can also copy data from device to host.\n",
    "cudaMemcpy(host_a, device_a, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "verifyOnHost(host_a, N);\n",
    "\n",
    "cudaFree(device_a);\n",
    "cudaFreeHost(host_a);          // Free pinned memory like this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：手动分配主机和设备内存\n",
    "\n",
    "向量加法应用程序[01-stream-init-solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu)的最新迭代使用 `cudaMallocManaged` 命令分配受管理的内存，该内存首先由初始化核函数在设备上使用，然后由向量加法核函数在设备上使用，然后再由主机所使用以对加法结果做验证。此时内存数据的传输是自动进行的。这是种很明智的方法，但我们也值得尝试一些手动内存分配和拷贝方法，以观察其对应用程序性能的影响。\n",
    "\n",
    "将 [01-stream-init-solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu) 应用程序重构为**不**使用 `cudaMallocManaged` 命令。为此，您需要执行以下操作：\n",
    "\n",
    "- 将调用 `cudaMallocManaged` 命令替换为调用 `cudaMalloc` 命令。\n",
    "- 创建将用于在主机上验证的额外向量。这是由于使用 `cudaMalloc` 命令分配的内存在主机上不可用，因此您必须执行此操作, 使用 `cudaMallocHost` 命令分配此主机向量。\n",
    "- 在 `addVectorsInto` 核函数运行完毕后，使用 `cudaMemcpy` 命令将包含相加结果的向量复制到使用 `cudaMallocHost` 命令创建的主机向量中。\n",
    "- 使用 `cudaFreeHost` 命令释放经由 `cudaMallocHost` 命令分配的内存。\n",
    "\n",
    "如您遇到问题，请参阅 [解决方案](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-manual-alloc 06-stream-init/solutions/01-stream-init-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成重构后，在Nsight Systems中打开一个报告，并使用活动时间表执行以下操作：\n",
    "\n",
    "- 注意，时间表的*统一内存*部分将不复存在。\n",
    "- 比较此时间表与之前重构的时间表，并使用时间轴标尺比较当前应用程序中 `cudaMalloc` 的运行时间与先前应用程序中 `cudaMallocManaged` 的运行时间。\n",
    "- 查看当前应用程序中初始化核函数的运行开始时间如何会晚于其在上次迭代中的运行时间。检查过时间轴后，您将发现时间差在于 `cudaMallocHost` 命令所用的时间。这很清楚地表明内存传输与内存拷贝的区别。正如您当前的操作，拷贝内存时，数据将存在于系统中的 2 个不同位置。与在上次迭代中仅分配 3 个向量相比，当前分配第 4 个主机向量会产生较小的性能成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 使用流实现数据传输和代码的重叠执行\n",
    "\n",
    "以下幻灯片概要地讲解了后面将涉及的内容。请点击浏览一遍这些幻灯片，然后再继续深入了解以下章节中的主题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-3-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了`cudaMemcpy`之外，还有`cudaMemcpyAsync`，只要固定了主机内存，它就可以从主机到设备或从设备到主机异步复制内存，这可以通过使用`cudaMallocHost`分配它来完成。\n",
    "\n",
    "与核函数的执行类似，`cudaMemcpyAsync`在默认情况下仅相对于主机是异步的。默认情况下，它在默认流中执行，因此对于GPU上发生的其他CUDA操作而言，它是阻塞操作。但是，`cudaMemcpyAsync`函数将非默认流作为可选的第5个参数。通过向其传递非默认流，可以将内存传输与其他非默认流中发生的其他CUDA操作并发。\n",
    "\n",
    "一种常见且有用的模式是结合使用固定主机内存，非默认流中的异步内存副本和非默认流中的核函数执行，以使内存传输与核函数的执行重叠。\n",
    "\n",
    "在以下示例中，我们并非在等待整个内存拷贝完成之后再开始运行核函数，而是拷贝并处理所需的数据段，并让每个拷贝/处理中的数据段均在各自的非默认流中运行。通过使用此技术，您可以开始处理部分数据，同时为后续段并发执行内存传输。使用此技术计算操作次数的数据段特定值和数组内的偏移位置时必须格外小心，如下所示：\n",
    "\n",
    "```cpp\n",
    "int N = 2<<24;\n",
    "int size = N * sizeof(int);\n",
    "\n",
    "int *host_array;\n",
    "int *device_array;\n",
    "\n",
    "cudaMallocHost(&host_array, size);               // Pinned host memory allocation.\n",
    "cudaMalloc(&device_array, size);                 // Allocation directly on the active GPU device.\n",
    "\n",
    "initializeData(host_array, N);                   // Assume this application needs to initialize on the host.\n",
    "\n",
    "const int numberOfSegments = 4;                  // This example demonstrates slicing the work into 4 segments.\n",
    "int segmentN = N / numberOfSegments;             // A value for a segment's worth of `N` is needed.\n",
    "size_t segmentSize = size / numberOfSegments;    // A value for a segment's worth of `size` is needed.\n",
    "\n",
    "// For each of the 4 segments...\n",
    "for (int i = 0; i < numberOfSegments; ++i)\n",
    "{\n",
    "  // Calculate the index where this particular segment should operate within the larger arrays.\n",
    "  segmentOffset = i * segmentN;\n",
    "\n",
    "  // Create a stream for this segment's worth of copy and work.\n",
    "  cudaStream_t stream;\n",
    "  cudaStreamCreate(&stream);\n",
    "  \n",
    "  // Asynchronously copy segment's worth of pinned host memory to device over non-default stream.\n",
    "  cudaMemcpyAsync(&device_array[segmentOffset],  // Take care to access correct location in array.\n",
    "                  &host_array[segmentOffset],    // Take care to access correct location in array.\n",
    "                  segmentSize,                   // Only copy a segment's worth of memory.\n",
    "                  cudaMemcpyHostToDevice,\n",
    "                  stream);                       // Provide optional argument for non-default stream.\n",
    "                  \n",
    "  // Execute segment's worth of work over same non-default stream as memory copy.\n",
    "  kernel<<<number_of_blocks, threads_per_block, 0, stream>>>(&device_array[segmentOffset], segmentN);\n",
    "  \n",
    "  // `cudaStreamDestroy` will return immediately (is non-blocking), but will not actually destroy stream until\n",
    "  // all stream operations are complete.\n",
    "  cudaStreamDestroy(stream);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：核函数和内存复制回主机重叠执行\n",
    "\n",
    "向量加法应用程序 [01-manual-malloc-solution.cu](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) 的最新迭代目前正在 GPU 上执行所有向量加法操作，完成后其会将内存拷贝回主机以进行验证。\n",
    "\n",
    "重构 [01-manual-malloc-solution.cu](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) 应用程序，在非默认流中分4段执行矢量加法，以便可以在等待所有向量加法工作完成之前开始异步内存复制。如您遇到问题，请参阅 [解决方案](../edit/08-overlap-xfer/solutions/01-overlap-xfer-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "assessment  assessment_results\ttask\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-manual-alloc 07-manual-malloc/solutions/01-manual-malloc-solution.cu -run\n",
    "!tar zcvf task.gz ../task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成重构后，在Nsight Systems中打开一个报告，并使用活动时间表执行以下操作：\n",
    "\n",
    "- 记录设备到主机的内存传输开始时间是在所有核函数工作完成之前还是之后？\n",
    "- 需注意 4 个内存拷贝段本身并不重叠。即使是在单独的非默认流中，在给定方向（此处为 DtoH）上，每次也只能同时进行一个内存传输。此处获得性能提升的原因在于其能先于其他方式开始内存传输，并且不难想象：若在某个应用程序中所完成的工作量与简单的加法运算相比，并非可以几乎忽略不计，那么内存拷贝不仅开始得更早，而且还会与核函数的执行相重叠。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
