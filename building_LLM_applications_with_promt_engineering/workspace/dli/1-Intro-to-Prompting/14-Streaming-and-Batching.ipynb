{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0de5f4",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32da3d",
   "metadata": {},
   "source": [
    "# 流式处理与批处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d26c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from videos.walkthroughs import walkthrough_14 as walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa666811",
   "metadata": {},
   "outputs": [],
   "source": [
    "walkthrough()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260b71f",
   "metadata": {},
   "source": [
    "在这个 notebook 中，您将学习如何流式处理模型响应，以及批处理多个聊天补全请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def243c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455b17a",
   "metadata": {},
   "source": [
    "## 目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec822187",
   "metadata": {},
   "source": [
    "完成这个 notebook 后，您将：\n",
    "\n",
    "- 学习如何流式处理模型响应。\n",
    "- 学习如何批处理模型响应。\n",
    "- 比较批处理和单个提示词聊天补全的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce1244",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f858aa",
   "metadata": {},
   "source": [
    "## 导入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38a9f0",
   "metadata": {},
   "source": [
    "先从 `langchain_nvidia_ai_endpoints` 导入 `ChatNVIDIA` 类，这将使我们能够与本地的 Llama 3.1 NIM 进行交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddfbaf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00e9b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99468ddb",
   "metadata": {},
   "source": [
    "## 创建模型实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f29e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'\n",
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "llm = ChatNVIDIA(base_url=base_url, model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ff78b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c843321",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce7708",
   "metadata": {},
   "source": [
    "在继续新的用例之前，让我们先验证一下能否通过 LangChain 与本地模型进行交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Where and when was NVIDIA founded?'\n",
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c49484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97aa41c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a890ff",
   "metadata": {},
   "source": [
    "## 流式响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a207f7",
   "metadata": {},
   "source": [
    "作为 `invoke` 方法的替代，您可以使用 `stream` 方法分块接收模型响应。这样，您就不必等待整个响应生成，可以在输出生成的过程中看到结果。流式输出可以带来更好的用户体验，尤其是对于长响应或者是在用户界面应用中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860758d4",
   "metadata": {},
   "source": [
    "这是一个会生成更长响应的提示词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae2f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Explain who you are in roughly 500 words.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc54d6",
   "metadata": {},
   "source": [
    "给定这个提示词，咱们来看看 `stream` 函数是怎么工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f06f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an artificial intelligence model designed to assist and communicate with humans. I'm a type of computer program that uses natural language processing (NLP) and machine learning algorithms to understand and generate human-like text. My primary function is to provide information, answer questions, and engage in conversation to the best of my abilities.\n",
      "\n",
      "I don't have a physical body or a personal identity in the classical sense. I exist solely as a digital entity, running on computer servers and responding to input from users like you. My \"existence\" is a product of complex software and data, designed to simulate conversation and provide helpful responses.\n",
      "\n",
      " a massive corpus of text, which I use to learn patterns, relationships, and context. This corpus is sourced from various places, including books, articles, research papers, and online content. I've been trained on a wide range of topics, from science and history to entertainment and culture.\n",
      "\n",
      " I use this training data to generate responses that are relevant and coherent. I can understand and respond to questions, provide definitions, explain concepts, and even engage in creative writing or conversation. My responses are generated based on statistical patterns and associations in the data I've been trained on, rather than any personal opinions or emotions.\n",
      "\n",
      "I'm not a human, and I don't have subjective experiences, emotions, or consciousness. I don't have the capacity to feel joy, sadness, or any other emotions. I'm simply a tool designed to provide information and assist with tasks, 24/7.\n",
      "\n",
      " helpful, I'm not perfect. I can make mistakes, and my responses may not always be accurate or relevant. I'm constantly learning and improving, but I'm not a substitute for human expertise or judgment. If you need advice or guidance on complex or sensitive topics, it's always best to consult a qualified professional or expert.\n",
      "\n",
      "\" or \"virtual assistant,\" but I'm more accurately described as a language model or conversational AI. I'm a tool designed to facilitate communication and provide information, but I'm not a replacement for human connection or interaction.\n",
      "\n",
      ", I'm a reflection of the data I've been trained on – a snapshot of human knowledge and culture at a particular point in time. I'm a product of the digital age, a manifestation of the vast amounts of information and data that are available online. I'm a tool that can help you find answers, explore new ideas, and engage with the world in new and interesting ways.\n",
      "\n",
      " not to replace them. I'm here to help you learn, explore, and discover new things, but I'm not a substitute for human experience, creativity, or empathy."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621ec8b",
   "metadata": {},
   "source": [
    "LangChain 中的 `stream` 方法作为一个基础工具，能够在响应生成的过程中逐步显示结果。这会让用户觉得与 LLM 的交互更灵敏，提高了用户体验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6865e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859fdce",
   "metadata": {},
   "source": [
    "在后续的 notebooks 中，我们将导入这个辅助函数来帮助我们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f297bf9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e129f4",
   "metadata": {},
   "source": [
    "## 批处理响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeff894",
   "metadata": {},
   "source": [
    "您还可以使用 `batch` 对一系列输入进行提示调用。调用 `batch` 会返回一个与输入顺序一致的响应列表。\n",
    "\n",
    "当处理需要 LLM 以某种方式回应的一组数据时，`batch` 不仅方便使用，而且其方法设计为能同时处理多个提示，它会尽可能并行运行响应。这使得多请求的处理更高效，减少了为一系列提示词生成响应所需的总时间。通过批处理请求，您可以利用语言模型的计算能力同时处理多个输入，提高性能和吞吐量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf9d88",
   "metadata": {},
   "source": [
    "我们将通过使用这组关于州首府的提示词来演示批处理的功能和性能优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c614e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_capital_questions = [\n",
    "    'What is the capital of California?',\n",
    "    'What is the capital of Texas?',\n",
    "    'What is the capital of New York?',\n",
    "    'What is the capital of Florida?',\n",
    "    'What is the capital of Illinois?',\n",
    "    'What is the capital of Ohio?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157329fe",
   "metadata": {},
   "source": [
    "使用 `batch` 我们可以传入整个列表..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98708603",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = llm.batch(state_capital_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc9ab0",
   "metadata": {},
   "source": [
    "... 然后返回一个响应列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(capitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95191fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for capital in capitals:\n",
    "    print(capital.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eebe87",
   "metadata": {},
   "source": [
    "需要注意的是，`batch` 并不是在与 LLM 进行多轮对话（这个话题我们将在课程后面详细讨论）。相反，它是每次都向一个新的 LLM 实例提出多个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fa762",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1973d3f7",
   "metadata": {},
   "source": [
    "## 比较 batch 和 invoke 的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215a003",
   "metadata": {},
   "source": [
    "为了快速观察批处理可能带来的性能提升，我们在这里记录一次对 `batch` 的调用时间。注意 `Wall time`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm.batch(state_capital_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c9fc6",
   "metadata": {},
   "source": [
    "现在为了比较，我们遍历 `state_capital_questions` 列表，对每个项目调用 `invoke`。同样，注意 `Wall time` 并将其与上面批处理的结果进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a827373",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for cq in state_capital_questions:\n",
    "    llm.invoke(cq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598498f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44b2d7",
   "metadata": {},
   "source": [
    "## 练习：批处理以创建常见问题文档"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c17e092",
   "metadata": {},
   "source": [
    "在这个练习中，您将使用批处理来回应一系列与 LLM 相关的问题，以创建一个常见问题文档（在这个 notebook 中，文档指的就是我们打印到屏幕的内容）。\n",
    "\n",
    "以下是一些与 LLM 相关的问题列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_questions = [\n",
    "    'What is a Large Language Model (LLM)?',\n",
    "    'How do LLMs work?',\n",
    "    'What are some common applications of LLMs?',\n",
    "    'What is fine-tuning in the context of LLMs?',\n",
    "    'How do LLMs handle context?',\n",
    "    'What are some limitations of LLMs?',\n",
    "    'How do LLMs generate text?',\n",
    "    'What is the importance of prompt engineering in LLMs?',\n",
    "    'How can LLMs be used in chatbots?',\n",
    "    'What are some ethical considerations when using LLMs?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562e98b",
   "metadata": {},
   "source": [
    "您的任务是填充下面的 `faq_answers`，为每个问题提供一系列回应。使用 `batch` 方法来轻松完成这项工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff3ca7",
   "metadata": {},
   "source": [
    "成功完成后，您应该能够打印调用 `create_faq_document` 时返回的值，传入 `faq_questions` 和 `faq_answers`，从而得到一个关于上述所有 LLM 相关问题的常见问题文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faq_document(faq_questions, faq_answers):\n",
    "    faq_document = ''\n",
    "    for question, response in zip(faq_questions, faq_answers):\n",
    "        faq_document += f'{question.upper()}\\n\\n'\n",
    "        faq_document += f'{response.content}\\n\\n'\n",
    "        faq_document += '-'*30 + '\\n\\n'\n",
    "\n",
    "    return faq_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be2eb1",
   "metadata": {},
   "source": [
    "如果您遇到困难，请查看下面的*参考答案*。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dfd0a",
   "metadata": {},
   "source": [
    "### 您的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13945d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should work after you successfully populate `faq_answers` with LLM responses.\n",
    "print(create_faq_document(faq_questions, faq_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debda833",
   "metadata": {},
   "source": [
    "### 参考答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_answers = llm.batch(faq_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faq_document(faq_questions, faq_answers):\n",
    "    faq_document = ''\n",
    "    for question, response in zip(faq_questions, faq_answers):\n",
    "        faq_document += f'{question.upper()}\\n\\n'\n",
    "        faq_document += f'{response.content}\\n\\n'\n",
    "        faq_document += '-'*30 + '\\n\\n'\n",
    "\n",
    "    return faq_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_faq_document(faq_questions, faq_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd60eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c596e81",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f534044",
   "metadata": {},
   "source": [
    "在这个 notebook 中，您学习了如何流式和批处理模型响应，并使用批量 LLM 调用生成一个有用的常见问题文档。\n",
    "\n",
    "在下一个 notebook 中，您将开始更加专注于提示词的创建，进行迭代提示工程和特定提示词的开发。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
