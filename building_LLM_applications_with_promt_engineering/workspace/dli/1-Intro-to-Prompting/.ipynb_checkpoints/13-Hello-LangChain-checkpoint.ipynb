{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3234f669",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc67680",
   "metadata": {},
   "source": [
    "# LangChain Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from videos.walkthroughs import walkthrough_13 as walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effea320",
   "metadata": {},
   "outputs": [],
   "source": [
    "walkthrough()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cfe5da",
   "metadata": {},
   "source": [
    "在这个 notebook 中，我们将学习如何与 LangChain 交互，以使用 Llama 3.1 8b instruct 模型进行聊天补全。这项入门练习将帮您理解在 Jupyter 环境中设置和使用 LangChain 的基本知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda9bb6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558c54e",
   "metadata": {},
   "source": [
    "## 目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22553da7",
   "metadata": {},
   "source": [
    "在您完成这个 notebook 时，您将：\n",
    "\n",
    "- 对 LangChain 有一个初步了解。\n",
    "- 使用 LangChain 进行简单的聊天补全。\n",
    "- 比较使用 LangChain 和 OpenAI 库进行聊天补全之间的区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca4e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548ec32",
   "metadata": {},
   "source": [
    "## 导入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3f243",
   "metadata": {},
   "source": [
    "从 `langchain_nvidia_ai_endpoints` 导入 `ChatNVIDIA` 类，这将使我们能够与本地的 Llama 3.1 8b instruct NIM 进行交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02baf6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c527d2",
   "metadata": {},
   "source": [
    "## 使用 langchain_nvidia_ai_endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca9a72",
   "metadata": {},
   "source": [
    "正如您在上一个 notebook 中观察到的，使用 OpenAI 的完成可能会导致代码重复。\n",
    "\n",
    "开发者们在高效利用 AI 应用方面付出了很多努力。其中，[LangChain](https://python.langchain.com/v0.2/docs/introduction/) 是一个流行的 LLM 编排框架，它帮助用户轻松地与 LLM 进行交互。\n",
    "\n",
    "LangChain 简单的架构和抽象使开发者能够轻松地替换语言模型、提示词和处理步骤，几乎不需要修改。此外，LangChain 在不同提供商的多个 LLM 之间提供了一致的统一接口，简化了交互，让开发者能专注于应用开发，而不是处理特定模型的复杂性。\n",
    "\n",
    "这个库非常受欢迎，并且随着该领域的进展迅速发展。虽然 LangChain 有很多组成部分，比如 LangGraph、LangSmith 和 LangServe，但我们今天的课程将重点关注 LangChain 核心部分。\n",
    "\n",
    "为了将 LangChain 与我们本地托管的模型配合使用，需要利用一个框架连接器，它会将任意 API 转换为目标代码库期望的形式。我们可以通过 `langchain-nvidia-ai-endpoints` 包中的 `ChatNVIDIA` 类来做到这一点。通过这个在后台用 OpenAI API 的工具，就可以更高效地迭代开发和测试提示词，并将 LangChain 与我们的 NVIDIA NIM LLM 一起使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2d18d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddde7c",
   "metadata": {},
   "source": [
    "## 使用 LangChain 设置模型实例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db0d7b",
   "metadata": {},
   "source": [
    "要开始使用 LangChain，我们需要设置 ChatNVIDIA 模型实例。这涉及到配置基本 URL 和模型名称，和我们在上一个 notebook 中使用 `OpenAI` 库时做的类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'\n",
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "llm = ChatNVIDIA(base_url=base_url, model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0d55e",
   "metadata": {},
   "source": [
    "您可能已经注意到我们将一个名为 `temperature` 的值设置为 `0`。`temperature` 是一个介于 `0` 和 `1` 之间的浮点值，用于控制模型响应的随机性。当设置为 `0` 时，LLM 会始终生成它认为下一个最有可能出现的文本。当设置为更高的值时，它可能会生成一些不是最高分的文本，因此便引入了随机性和创造性。\n",
    "\n",
    "我们将在课程后面详细讨论 `temperature` 的使用，但目前要知道设置为 `0` 是让 LLM 的响应变得确定：给定相同的提示词，它将始终以相同的方式响应。\n",
    "\n",
    "对于那些有兴趣进一步了解 temperature 和其它额外超参数工作原理的学员，欢迎查看位于本目录下的 [99-Appendix-Hyperparams](99-Appendix-Hyperparams.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69093519",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8349a8",
   "metadata": {},
   "source": [
    "## 发起简单请求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e274c0",
   "metadata": {},
   "source": [
    "现在可以开始向模型发送聊天补全提示词了。我们将首先使用 `invoke` 方法，希望您也会觉得它比上一个 notebook 中的 OpenAI 客户端容易得多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Who are you?'\n",
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2819b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bdfbc2",
   "metadata": {},
   "source": [
    "结果与我们使用 OpenAI 客户端获得的类似，但它还包含关于对话和 token 的元数据。这对于在更高级的应用中维护对话上下文非常有用。\n",
    "\n",
    "要从模型中仅提取响应内容，我们只需使用结果的 `content` 属性，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b57b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b62ec5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784833f",
   "metadata": {},
   "source": [
    "## 练习：生成您自己的完成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62538b0f",
   "metadata": {},
   "source": [
    "使用我们现有的模型实例 `llm` 来生成并打印本地 Llama 3.1 模型的响应，用您自己些的提示词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff090c",
   "metadata": {},
   "source": [
    "### 您的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd4f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "774c0a75",
   "metadata": {},
   "source": [
    "### 参考答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7417039",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Give me 3 puns having to do with LangChain.'\n",
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f635e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a44823",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f821f",
   "metadata": {},
   "source": [
    "通过完成这个 notebook，您现在应该对如何使用 LangChain 进行聊天补全和解析模型响应有了基本的理解，您应该也会觉得这其实很简单。\n",
    "\n",
    "下一个 notebook 中，您将进一步学习如何使用 LangChain 进行聊天补全，包括如何流式传输模型响应和批量处理多个聊天补全请求。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
