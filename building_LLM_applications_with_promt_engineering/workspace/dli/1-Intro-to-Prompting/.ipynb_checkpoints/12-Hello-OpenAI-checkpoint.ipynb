{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf808569",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983e707",
   "metadata": {},
   "source": [
    "# OpenAI 库 Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from videos.walkthroughs import walkthrough_12 as walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c489679",
   "metadata": {},
   "outputs": [],
   "source": [
    "walkthrough()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda9a4b",
   "metadata": {},
   "source": [
    "在这个 notebook 中，我们将学习如何与 OpenAI API 交互，通过 Llama 3.1 8b 模型进行文本补全（text completion）。本节会介绍如何设置和使用 OpenAI 库以与 LLM 交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a41297",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccead7f",
   "metadata": {},
   "source": [
    "## 目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47396d",
   "metadata": {},
   "source": [
    "完成这个 notebook 后，您将会：\n",
    "\n",
    "- 理解如何设置和使用 OpenAI 库。\n",
    "- 使用 Llama 3.1 8b instruct 模型进行文本补全。\n",
    "- 学会解释和利用 API 响应。\n",
    "- 理解在像 Llama 3.1 8b instruct 这样的聊天模型中使用 *chat* 补全入口的重要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de837532",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c3325",
   "metadata": {},
   "source": [
    "## 导入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d66ad",
   "metadata": {},
   "source": [
    "在这里我们导入 `OpenAI` 库，它将使我们能够与本地托管的 Llama 3.1 8b Instruct NIM 交互，该 NIM 暴露了 OpenAI API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0870947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea40b9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4614a93",
   "metadata": {},
   "source": [
    "## 设置 OpenAI 客户端"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455a419",
   "metadata": {},
   "source": [
    "要开始使用 OpenAI API，我们需要设置 OpenAI 客户端。这涉及到配置基础 URL 和提供 API 密钥。\n",
    "\n",
    "默认情况下，OpenAI API 服务器监听 `8000` 端口并暴露 `/v1` 入口。在我们的情况下，我们有一个本地运行的 NIM，与您正在交互的 Jupyter 环境在同一台机器上，NIM 的主机名是 `llama`。因此，为了构造用于与 NIM 交互的 `base_url`，我们将使用 `llama` 主机名与 `8000` 端口和 `/v1` 入口结合起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd479b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400f656",
   "metadata": {},
   "source": [
    "创建 OpenAI 客户端时，`api_key` 参数是必需的，但在我们本地运行模型的情况下，实际上并不需要提供 API 密钥。因此我们将把 `api_key` 的值设置为一个任意字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'an_arbitrary_string'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e7f622",
   "metadata": {},
   "source": [
    "现在有了 `base_url` 和 `api_key`，我们可以实例化一个 OpenAI 客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=base_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be2fd7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787b87c",
   "metadata": {},
   "source": [
    "## 观察可用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8d61a",
   "metadata": {},
   "source": [
    "现在我们已经创建了 OpenAI 客户端，可以先通过调用 `client.models.list()` 来看看有哪些能用的模型。正如之前提到的，我们需要一个 Llama 3.1 8B Instruct 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abeb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a5c33",
   "metadata": {},
   "source": [
    "这里有很多信息我们并不关心，稍微深入一下这个对象就能更清楚地看到可用的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f761d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models.data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bc3b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd9d82",
   "metadata": {},
   "source": [
    "## 发起简单的聊天补全请求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70f9f2",
   "metadata": {},
   "source": [
    "创建了 `client` 实例后，我们可以通过使用 `client.chat.completions.create` 方法发起一个简单的请求来实现聊天补全，该方法需要用到 `model`，以及一组要发送给模型的 `messages`。关于 `messages` 列表的细节稍后会详细讨论，现在我们将传入一个简单的单条消息，是一个用户（您）要求模型讲一个关于太空的有趣事实的提示词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "prompt = 'Tell me a fun fact about space.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60606797",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15a4da",
   "metadata": {},
   "source": [
    "API 响应中提供了相当多的信息，但我们最关心的是模型的响应。\n",
    "\n",
    "在这里，我们从完整的 API 响应中解析出模型生成的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07daa373",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49deb338",
   "metadata": {},
   "source": [
    "## 练习：创建您的第一个提示词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be16fa5",
   "metadata": {},
   "source": [
    "使用我们现有的 OpenAI API `client` 来生成并打印一个本地 Llama 3.1 8b 模型的响应，用您自己的提示词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046a10a",
   "metadata": {},
   "source": [
    "### 您的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aeca0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb473e08",
   "metadata": {},
   "source": [
    "### 参考答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What is the OpenAI API?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e0481",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e024f",
   "metadata": {},
   "source": [
    "## 理解补全和聊天补全入口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae9f0e",
   "metadata": {},
   "source": [
    "我们一直在使用 `chat.completions` 入口，但在使用 OpenAI API 时，您也可以选择使用 `completions` 入口。理解这些入口之间的差异非常重要，因为它们处理提示词和生成响应的方式不同，即使是对于单个提示词。\n",
    "\n",
    "`chat.completions` 入口旨在处理多轮对话，跟踪先前消息提供的上下文。通过预测交互，它生成更简洁、切中主题的响应，即使只提供了单个提示词。\n",
    "\n",
    "而 `completions` 入口则是为了生成针对单条提示词的响应，不维持对话上下文。它的目标是回应给定的提示词，而不是以对话的方式进行响应。\n",
    "\n",
    "主要的要点是，当您使用“聊天”或“指令”模型（比如您今天使用的 llama-3.1-8b-instruct 模型）时，请使用 `chat.completions` 而不是 `completions`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0339731",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676caba",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144941ab",
   "metadata": {},
   "source": [
    "完成这个 notebook 后，您应该对如何使用 OpenAI 库进行聊天补全并解析模型响应有了基本的了解。为接下来更高级的主题和提示工程打下了基础。\n",
    "\n",
    "下一个 notebook 中，我们将探讨如何使用 LangChain 与语言模型交互，这将为管理和生成文本提供更多灵活性和高级功能。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
